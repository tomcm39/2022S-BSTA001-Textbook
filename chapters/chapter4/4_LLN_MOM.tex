\chapterauthor{thomas mcandrew}{Lehigh University}
%\chapterauthor{Second Author}{Second Author Affiliation}
\chapter{Law of large numbers, Method of Moments, and the Central limit theorem}
\hspace{1mm}

\section{Introduction}\label{intro}

The law of large numbers~(LLN) and the central limit theorem underpin a large portion of statistical theory. 
We will see that the LLN will link proportions to probabilities, expected values to the sample mean, and allow us to estimate parameters of a random variable given a dataset.

The method of moments will be the primary tool we use to derive parameter estimates for parameters for a given distribution of a random variable. This method follows directly from the LLN.

Finally, we will explore the Central limit theorem which is the foundation of much of  generating hypothesis tests and building confidence intervals. 

\section{Independent and identically distributed and sample}

To model a dataset, $\mathcal{D}$, we will need to make assumptions about how our data set was generated.

A common assumption for a dataset with $n$ data points $\mathcal{D} = (x_{1}, x_{2}, \cdots, x_{n} )$ is that each data point was generated from a random variable $x_{1} \sim X_{1}, x_{2} \sim X_{2}, \cdots, x_{n} \sim X_{n}$. 
The collection of random variables $X_{1},X_{2},\cdots, X_{n})$ is called a \textbf{sample} and the dataset $\mathcal{D}$ above is typically called one \textbf{realized sample} or a \textbf{realization} of the sample.

If we additionally assume that all the random variables are \textbf{identically distributed}, or that they have the same distribution $X_{1},X_{2}, \cdots, X_{n} \sim f$, and we assume that for any $i$ and $j$ that $X_{i}$ and $X_{j}$ are independent then we call our sample a \textbf{random sample}.

Because pairwise independence between random variables and that all random variables are identically distributed are two common assumptions, they are abbreviated as \textbf{i.i.d} which stands for "independent and identically distributed".

\section{Properties of the Expected value and Variance of a sum}

To explore the law of large numbers, we will need another property of the expectation.
The expected value of the sum of random variables is the sum of the expected value of each individual random variable. 
\begin{align}
    \mathbb{E}(X+Y) = \mathbb{E}(X) + \mathbb{E}(Y)
\end{align}

When we combine the properties of expected values we have already learned, we can state 
\begin{align}
    \mathbb{E}(aX+bY) = a \mathbb{E}(X) + b \mathbb{E}(Y)
\end{align}
and this is true for any number of random variables
\begin{align}
    \mathbb{E}\left( \sum_{i=1}^{N} a_{i} X_{i} \right) = a_{i} \sum_{i=1}^{N} \mathbb{E}(X_{i})
\end{align}

We start at the definition of the expected value to derive this property 
\begin{align}
    \mathbb{E}(X+Y) &= \sum_{x} \sum_{y} (X+Y) f_{X+Y}(x,y) \\ 
                    &=\sum_{x} \sum_{y} X f_{X+Y}(x,y) + Y f_{X+Y}(x,y) \\
                    &= \sum_{x} \sum_{y} X f_{X+Y}(x,y) +  \sum_{x} \sum_{y} Y f_{X+Y}(x,y) \\
                    &= \sum_{x} X f_{X}(x) + \sum_{y} Y f_{Y}(y) \\
                    &= \mathbb{E}(X) + \mathbb{E}(Y)
\end{align}

\ex Suppose $X \sim \text{Geom}(1/4)$ and $Y \sim \text{Bernoulli}(1/2)$ then $\mathbb{E}(X+Y) = \mathbb{E}(X) + \mathbb{E}(Y) = \frac{1}{1/4} + \frac{1}{2} = 4 \frac{1}{2}$. 

The variance is an expected value and so it follows that 
\begin{align}
    V(X+Y) = V(X) + V(Y)
\end{align}
and we can state more generally that for two independent random variables 
\begin{align}
    V(aX+bY) = a^{2}V(X) + b^{2}V(Y)
\end{align}

\ex Let $X \sim \text{Binomial}(N,\theta)$ and $Y \sim \text{Binomial}(M,\gamma)$ then $V(X-Y) = V(X + (-1) Y) = V(X) + (-1)^{2} V(Y) = N \theta (1-\theta) + M \gamma (1-\gamma)$.

\section{Law of large numbers}\label{intro}

The intuition behind the law of large numbers can be characterized by the following experiment: you are asked to flip a fair coin and record the whether the coin is heads up or tails up. After 10 flips you are asked to compute the proportion of heads up flips, after 50 flips you are asked to compute this proportion, after 100, 1,000, 10,000 flips you are asked to compute the proportion of heads up flips. We expect that if this coin is fair that the proportion of flips with heads face up will get closer and closer to 0.50. 

The law of large numbers attempts to describe this intuition.

Define a sequence of random variables $X_{1}, X_{2}, \cdots, X_{n}$ such that any pair of random variables, $X_{i}$ and $X_{j}$ are independent (that is $P(X_{i} = x | X_{j} = y) =P(X_{i} = x)$. Finally, transform this sequence into a single random variable $\overline{X}$ that is equal to $\overline{X} = \dfrac{X_{1} + X_{2} + X_{3} + \cdots X_{N}}{N}$. 

Then the law of large numbers~(LLN) states that given any small number $\epsilon$ that is greater than 0 as $n$ grows towards infinity~$(n \to \infty)$
\begin{align}
    P( | \overline{X}_{n} - \mu | > \epsilon ) \to 0
\end{align}
where $\mu = \mathbb{E}(\overline{X})$.

We can picture a distribution $Z_{n} = | \overline{X}_{n} - \mu |$ that depends on $n$ and as $n$ increase the random variable $Z_{n}$ assigns more and more probability to the value 0.
<pic>

The LLN has many applications. 

\begin{VT1}
\VH{Proportions approximate probabilities}
Let a sequence of Bernoulli distributed random variables be (i) pairwise independent and (ii) be distributed the same $X_{1}, X_{2}, \cdots, X_{N} \sim \text{Bern}(\theta)$.

Lets take a look at the transformation $\overline{X}$.
Consider a sequence of two random variables with a Bernoulli distribution. Then define
\begin{align}
    S = X_{1} + X_{2}
\end{align}

The expected value of $S$ is 

\begin{align}
    \mathbb{E}(S) &= \mathbb{E}(X_{1} + X_{2}) \\
                  &= \mathbb{E}(X_{1}) + \mathbb{E}(X_{2})\\
                  &= \theta + \theta \\ 
                  &= 2\theta
\end{align}
and the expected value of $\overline{X}$ is then 
\begin{align}
    \mathbb{E}(\overline{X}) = \frac{\mathbb{E}(S)}{2}  = \frac{2 \theta}{2} = \theta
\end{align}

To gain intuition about what a sample from $\overline{X}$ looks like, assume we collect data points from $n$ random variables $x_{1} \sim X_{1}, x_{2} \sim X_{2}, \cdots, x_{n} \sim X_{n}$.
An example of a single data point $d$ could be
\begin{align}
    d = (0,1,0,1,0,0,1,1,1,0)
\end{align}
and the sample mean $\overline{x}$, one sample from $\overline{X}$, is then 
\begin{align}
    S &= \sum_{i=1}^{10} d_{i} \\
    \overline{x} &= \frac{X}{n}
\end{align}

However, the value of $S$ can be counted in two different ways. 
We can sum up $d_{i}$ one by one in order, or we can multiply zero by the frequency of zeros in our dataset, multiply 1 by the frequency of ones in our dataset, and sum both of these products.
\begin{align}
    S &= \sum_{i=1}^{10} d_{i} \\
      &= 0+1+0+1+0+0+1+1+1+0\\
    S &= N(0) \cdot 0 + N(1) \cdot 1
\end{align}
where $N(x)$ is the number of values $x$ in our dataset.

If we use the above method to sum our data points the sample mean simplifies to  
\begin{align}
    \overline{x} &= \frac{ N(0) \cdot 0 + N(1) \cdot 1  }{N} \\ 
                 &= \frac{N(1)}{N}.
\end{align}
The sample mean is the proportion of 1s $(p_{n})$ from our sample of $n$ data points, and the law of large number then says that 
\begin{align}
    &P( | \overline{X} - \mu | > \epsilon ) \to 0 \\ 
    &P( | p_{n} - \theta | > \epsilon ) \to 0
\end{align}
or, roughly, that the sample proportion approaches $\theta$, the true probability that a $1$ will appear.

\end{VT1}


\begin{VT1}
\VH{The Relationship between the expected value and the sample mean}

Define a discrete random variable $X_{1}, X_{2}, \cdots, X_{n} \sim f_{X}$, $supp(X_{i}) = \{1,2,3\}$

Lets assume $n=10$ and take a closer look at $S$.
One sample from our $n=10$\ random variables could be  $d = (2,1,3,2,3,2,3,1,2,2)$ and we can compute the sum $S = 2+1+3+2+3+2+3+1+2+2$. 
However, we could have also summed these ten numbers by multiplying each unique number in the dataset by the frequency this number occurs and then summing these products: $S = N(1) \cdot 1 + N(2) \cdot 2 + N(3) \cdot 3$, where $N(x)$ is the number of times the value $x$ appears.
For our specific example $S = 2 \cdot 1 + 5 \cdot 2 + 3 \cdot 3$.
The sample mean is then 
\begin{align}
    \overline{x} &= \frac{S}{N} \\ 
                 &= \frac{N(1) \cdot 1 + N(2) \cdot 2 + N(3) \cdot 3}{N}\\
                 &= \frac{N(1)}{N} 1 + \frac{N(2)}{N} 2  + \frac{N(3)}{N} 3 
\end{align}
We know from our discussion of the LLN and Bernoulli distributed random variables that $\frac{N(1)}{N}$ is the proportion of $1$s , $p_{n}(1)$, and this proportion will approach the true probability of a 1 as we collect more data (as $n \to \infty$).

Let's rewrite the above sample mean computation
\begin{align}
    \overline{x} &= \frac{N(1)}{N} 1 + \frac{N(2)}{N} 2  + \frac{N(3)}{N} 3 \\ 
                 &= p_{n}(1) 1 + p_{n}(2) 2  + p_{n}(3) 3 \\ 
\end{align}
As $n \to \infty$
\begin{align}
    \overline{x} &= \frac{N(1)}{N} 1 + \frac{N(2)}{N} 2  + \frac{N(3)}{N} 3 \\ 
                 &= f_{X}(1) 1 + f_{X}(2) 2  + f_{X}(3) 3
\end{align}
but the above is the expected value of $X$, $\mathbb{E}(X)$, and so as $n$ gets larger the sample mean will become closer to the true expected value.
\end{VT1}


\subsection{LLN for transformations}

Earlier we saw that by definition if we define a random variable $X$ and a function $g$ then
\begin{align}
    \mathbb{E}\left[ g(X) \right] = \sum_{i \in supp(X)} g(x_{i}) f_{X}(x_{i})
\end{align}
Because the above is equivalent to the expectation of a transformed random variable $Y = g(X)$ the LLN has a similar result for any transformation of a random variable. 

As $n \to \infty$,
\begin{align}
    P( | \overline{Y}_{n} - \mu_{Y}| > \epsilon ) \to 0 
\end{align}
where $\mu_{Y} = \mathbb{E}(Y) = \mathbb{E}\left[g(X)\right]$.

The above implies that the LLN applies to any transformation of a random variable, including 
\begin{align}
    S = \frac{\sum_{i=1}^{N} ( X_{i} - \mu  )^{2}}{N}
\end{align}
where $\mu = \mathbb{E}[g(X)]$. 

Let us find a simple expression for $\mathbb{E}(S)$. 
\begin{align}
    \mathbb{E}(S) &= \mathbb{E} \left[ \frac{\sum_{i=1}^{N} ( X_{i} - \mu  )^{2}}{N}\right] \\ 
                  &= \frac{1}{N} \sum_{i=1}^{N} \mathbb{E}\left[( X_{i} - \mu  )^{2}\right]\\
                  &= \frac{1}{N} \sum_{i=1}^{N} V(X)\\
                  &= \frac{1}{N}  N V(X)  = V(X)
\end{align}

Because $\mathbb{E}(S) = V(X)$, the LLN states that for any $\epsilon >0$ as $n \to \infty$
\begin{align}
    P(|S - V(X)| > \epsilon) \to 0
\end{align}
or that $S$ approaches the true variance of $X$.


\section{Statistics, Estimators, and the Method of Moments}\label{intro}

Given a random sample $(X_{1}, X_{2}, X_{3}, \cdots, X_{n})$ or collection of random variables that are i.i.d, a  \textbf{statistic} is a function of our sample $T(X_{1},X_{2},X_{3},\cdots,X_{n})$.
Because a statistic is a function of a collection of random variables, our statistic too can be characterized by a probability distribution over potential values of the statistic.

For a random sample $(X_{1},X_{2},X_{3},\cdots,X_{n})$ all with the same probability density function $f_{X}(\theta)$ where $\theta$ is a parameter, an $estimator$, $T(X_{1},X_{2},\cdots,X_{n})$ of $\theta$ is (i) a statistic that (ii) is meant get closer to $\theta$ as $n \to \infty$.

An \textbf{estimate} is an estimator applied to a realized sample, and an estimate for a parameter is often denoted as the parameter with a "hat". For example, an estimate of the parameter $\beta$ would bve denoted $\hat{\beta}$.

One way to develop an estimator for a parameter, or for several parameters, is to use the \textbf{Method of Moments}.
For a random sample $(X_{1},X_{2},\cdots,X_{n})$ where each random variable follows the same probability density function with $k$ parameters $X_{i} \sim f_{X}(x | \theta_{1},\theta_{2},\theta_{k})$, the Method of moments generates estimators for all $k$ parameters by solving this set of equations 
\begin{align}
    \mathbb{E}(X)     &= g_{1}(\theta_{1},\theta_{2},\cdots,\theta_{k}) \\ 
    \mathbb{E}(X^{2}) &= g_{2}(\theta_{1},\theta_{2},\cdots,\theta_{k}) \\ 
    \vdots \\
    \mathbb{E}(X^{k}) &= g_{k}(\theta_{1},\theta_{2},\cdots,\theta_{k}), 
\end{align}
replacing each exact, expected value by sample means. 
The expected value $\mathbb{E}(X)$ is replaced by $\overline{X} = \sum_{i=1}^{N} X_{i}/N$, the expected value $\mathbb{E}(X^{2})$ is replaced by its sample mean $\overline{X^{2}} = \sum_{i=1}^{N} X_{i}^{2} / N$, etc. 

Lets look at an example. 


\begin{VT1}
\VH{MoM estimator for the Bernoulli}

Suppose we are studying the probability of survival among patients who are treated with dialysis and experienced a myocardial infarction~(see \url{10.1056/NEJM199809173391203} for a manuscript on this topic). We collect data on 50 patients and model whether they survive or not as a random sample $(X_{1},X_{2},X_{3},\cdots,X_{50})$ where $X_{i} \sim \text{Bern}(\theta)$ and the value 1 represents survival and zero otherwise. We follow patients for a year and record a dataset $\mathcal{D} =(1,0,0,1,1,1,0,0,0,0,1,0,\cdots)$ that contains 36 ones and 14 zeros. 

Because the Bernoulli distribution has a single parameter we only need a single equation that related the expected value and our parameter $\theta$. Because the Bernoulli distribution has a single parameter we only need a single equation that related the expected value and our parameter $\theta$.

The expected value is $\matbb{E}(X) = \theta$ and so we can derive a MoM estimator by replacing the exact $\mathbb{E}(X)$ with the sample mean $\overline{X}$. Our estimator for $\theta$ is $\hat{\theta} = \overline{X} = \sum_{i=1}^{N} x_{i} / N = 36/50 = 72\%$.

\end{VT1}

\begin{VT1}
\VH{MoM estimator for the Uniform Continuous}

Suppose $(Y_{1},Y_{2},\cdots,Y_{10})$ is a random sample such that $Y_{i} \sim U(\alpha,\beta)$. We collect a dataset (realized sample) $\mathcal{D} = (0.22,0.45,0.12,0.32,0.56,0.73,0.91,0.51,0.11,0.67)$. 
To develop an estimate for $\alpha$ and an estimate for $\beta$ using the method of moments, we will need to look at the following two equations:
\begin{align}
    \mathbb{E}(X)     &= \frac{\alpha+\beta}{2}\\ 
    \mathbb{E}(X^{2}) &= g(\alpha,\beta) 
\end{align}

On first glance, the second equation sounds difficult to solve.
However, we found an expression for the variance that depends on $\mathbb{E}(X^{2})$ and on $\mathbb{E}(X)$ in chapter 2, exercise 9.

\begin{align}
    V(X) &= \mathbb{E}(X^2) - \left[\mathbb{E}(X)\right]^{2} \\
    \mathbb{E}(X^2) &= V(X) +  \left[\mathbb{E}(X)\right]^{2}  
\end{align}

We know that $V(X) = \frac{(b-a)^{2}}{12}$ and so 

\begin{align}
    \mathbb{E}(X^2) &= V(X) +  \left[\mathbb{E}(X)\right]^{2}  \\
                    &= \frac{(\beta-a)^{2}}{12} + \left[ \frac{(\alpha+\beta)}{2} \right]^{2} \\ 
                    &= \frac{\beta^{2} + \alpha^{2} - 2\alpha\beta}{12} + \frac{\alpha^{2}+\beta^{2}+2\alpha\beta}{4} \\ 
                    &= \frac{\beta^{2} + \alpha^{2} + \alpha\beta}{3}  
\end{align}

Now our MoM equations are 
\begin{align}
    \mathbb{E}(X)     &= \frac{\alpha+\beta}{2}\\ 
    \mathbb{E}(X^{2}) &=  \frac{\beta^{2} + \alpha^{2} + \alpha\beta}{3}
\end{align}
and we will replace $\mathbb{E}(X)$ with the sample mean $\overline{X}$ and replace $\mathbb{E}(X^{2})$ with $\overline{X^{2}}$, and then solve for $\alpha$ and $\beta$.  
\begin{align}
    \overline{X}     &= \frac{\alpha+\beta}{2}\\ 
    \overline{X^{2}} &=  \frac{\beta^{2} + \alpha^{2} + \alpha\beta}{3}
\end{align}

From the first equation we find $\alpha = 2\overline{X} - \beta$. 
We can plug the above into the second equation to find 

\begin{align}
    \overline{X^{2}} &=  \frac{\beta^{2} + (2\overline{X} - \beta)^{2} + (2\overline{X} - \beta)\beta}{3}\\
    3\overline{X^{2}} &= \beta^{2} + 4 \overline{X}^{2} + \beta^{2} - 4\overline{X} \beta + 2\overline{X} \beta - \beta^{2}\\
    3\overline{X^{2}} &= \beta^{2} + 4 \overline{X}^{2} - 2\overline{X} \beta \\  
    0&= \beta^{2} - (2\overline{X}) \beta + (4 \overline{X}^{2} - 3\overline{X^{2}})\\
    \hat{\beta} &= 2\overline{X} \pm \frac{\sqrt{ (2\overline{X})^{2} - 4(1)(4 \overline{X}^{2} - 3\overline{X^{2}}) } }{2} \\ 
    \hat{\beta} &= 2\overline{X} \pm \frac{\sqrt{ 4\overline{X}^{2} - (16 \overline{X}^{2} - 12\overline{X^{2}}) } }{2} \\
    \hat{\beta} &= 2\overline{X} \pm \frac{\sqrt{ - 12 \overline{X}^{2} + 12\overline{X^{2}} } }{2} \\
    \hat{\beta} &= 2\overline{X} \pm \frac{ 2\sqrt{3} \sqrt{    \overline{X^{2}} -\overline{X}^{2} } }{2} \\
    \hat{\beta} &= 2\overline{X} \pm \sqrt{3} \sqrt{    \overline{X^{2}} -\overline{X}^{2} } \\
\end{align}

and so for our problem above $\hat{\beta} = 1.37$ and so $\alpha = -0.45$.


\end{VT1}


\begin{VT1}
\VH{MoM estimator for the Poisson}

Suppose $(R_{1},R_{2},\cdots,R_{5))$ is a random sample such that $R_{i} \sim \text{Pois}(\lambda)$, and we are given the following dataset $\mathcal{D} = (3,7,10,12,45)$.

We can develop an estimator for $\lambda$ by using the LLN and MoM:
\begin{align}
    \frac{1}{n}\left(R_{1} + R_{2} + \cdots + R_{n}) \to \mathbb{E}(R)
\end{align}
as $n \to \infty$.

The MoM of moments estimator is 
\begin{align}
   \frac{1}{n}\left(r_{1} + r_{2} + \cdots + r_{n}) &\approx  \mathbb{E}(R) \\ 
   \frac{1}{n}\left(r_{1} + r_{2} + \cdots + r_{n}) &= \lambda \\ 
   \hat{\lambda} &= \frac{1}{n}\sum_{i=1}^{n} r_{i}
\end{align}
as $n \to \infty$.

In our example, our estimate for the true $\lambda$ would be 
\begin{align}
    \hat{\lambda} = \frac{1}{5} \left(3+7+10+12+45) = 15.4
\end{align}

We can compute an estimated expected value, variance, estimated probabilities, etc by replacing the our true parameter value $\lambda$ with our estimate $\hat{\lambda}$.
For example, an estimate for the expected value  is 
\begin{align}
    \hat{\mathbb{E}(R)} = \hat{\lambda} = 15.4,
\end{align}
an estimated probability mass function 
\begin{align}
    f_{R}(r; \hat{\lambda}) = e^{-\hat{\lambda}} \frac{\hat{\lambda}^{r}}{r!}
\end{align}


\end{VT1}


\section{Central limit theorem}\label{intro}

Given a random sample $(X_{1},X_{2},\cdots,X_{n})$, we used the LLN and  method of moments to produce point estimates---a single number---for a parameter that assigned probabilities to value of $X_{i}$.
However the LLN only guarantees that our estimator will grow closer to the true parameter value as $n \to \infty$. 
The LLN does not describe how close we are to the true parameter value for any specific $n$.

The Central limit theorem~(CLT) describes the statistical distribution of the sample mean $\bar{X}$, allowing us to describe how close we are to the true parameter value of interest. 

For a random sample $(X_{1},X_{2},\cdots,X_{n})$ the \textbf{Central Limit Theorem} states
\begin{align}
    \overline{X}_{n} \sim \mathcal{N}\left(\mu,\frac{\sigma^{2}}{n}\right)
\end{align}

or that 
\begin{align}
    \sum_{i=1}^{n} X_{n} \sim \mathcal{N}\left(n \mu, n \sigma^{2}\right)
\end{align}


where $\overline{X}_{n} =  \frac{1}{n}\left(X_{1}+X_{2}+\cdots+X_{n}\right)$, $\mu = \mathbb{E}(X)$ and $\sigma^{2} = V(X)$.

\begin{VT1}
\VH{CLT for the Bernoulli}

Suppose we wish to study a random sample of $n$ Bernoulli distributed random variables $(Y_{1},Y_{2},\cdots,Y_{n}) \sim \text{Bern}(\theta)$.

We know that $\mathbb{E}(Y) = \theta$ and $V(Y) = \theta(1-\theta)$, and so 
\begin{align}
    \overline{Y}_{n} \sim \mathcal{N}\left(\theta, \frac{\theta (1-\theta)}{n} \right)
\end{align}

We saw in the previous section on the LLN and MoM that an estimate for $\theta$ is the sample mean so then 
\begin{align}
    \hat{\theta} \sim \mathcal{N}\left(\theta, \frac{\theta (1-\theta)}{n} \right)
\end{align}
However, we still cannot compute probabilities that $\hat{\theta}$ is in some interval because the Normal distribution on the right hand side involves the true, unknowable, $\theta$.

\begin{align}
    \hat{\theta}(Y) - \theta \sim \mathcal{N}\left(0, \frac{\theta (1-\theta)}{n} \right)
\end{align}
here we introduce the notation $\hat{\theta}(Y)$ to reinforce that this is a random variable and not a single value.

To approximate the difference $\hat{\theta}(Y) - \theta$ we can replace $\theta$ with our MoM estimate $\overline{y}$.
\begin{align}
    \hat{\theta}(Y) - \theta \sim \mathcal{N}\left(0, \frac{\overline{Y}_{n} (1-\overline{Y}_{n})}{n} \right)
\end{align}

We can use the above to approximate the probability that the difference between our estimated $\hat{\theta}$ and the true $\theta$ is inside some interval. 
Suppose we collect the data $\mathcal{D} = (1,1,0,1,0,1,1,1,0,0,1,1,0,1,0)$.
Then the CLT says 
\begin{align}
    \hat{\theta}(Y) - \theta \sim \mathcal{N}\left(0, \frac{\overline{y}_{n} (1-\overline{y}_{n})}{n} \right) \\ 
    \hat{\theta}(Y) - \theta \sim \mathcal{N}\left(0, \frac{ \frac{9}{15}  \cdot \frac{6}{15}} {15} \right) \\ 
\end{align}

and we can compute for example the probability that our estimate is 0.1 away from the truth. 
\begin{align}
    P( -0.1 <   \hat{\theta}(Y) - \theta < 0.1) = F(0.1) - F(-0.1) \approx 0.57
\end{align}
In other words, there is a $0.57$ probability that our estimate is within 0.1 of the true parameter value $\theta$.

\end{VT1}

\section{Confidence intervals: the CLT in action}

Given a random sample $(X_{1},X_{2},\cdots, X_{n},)$, a \textbf{confidence interval} for a parameter $\theta$ is a pair of statistics $(L(X),U(X))$ such that $P( \theta \in [L(X),U(X)] ) = 1 - \alpha$.
The number $\alpha$ is called the \textbf{confidence coefficient} and we often say that the interval $[L(X),U(X)]$ is a $1-\alpha$ confidence interval for the parameter $\theta$. 

A confidence interval provides a range of values that the true parameter is likely to be between.

\ex Suppose we decide to study the prevalence of tuberculosis among people who have diabetes mellitus. Our model supposes we can represent each person in our sample with a random variable $X \sim \text{Bern}(\theta)$ where $\theta$ is the probability of an active tuberculosis infection. Given a random sample  $(X_{1},X{2},\cdots,X_{n})$ where $X_{i} \sim \text{Bern}(\theta)$, we find 34 people had an active infection and 3,456 did not. If we generate a 95\% confidence interval for the parameter $\theta$ that is [0.05\%, 2\%] then there are tow claims. Claim one: The confidence interval applied to a random sample that follows all of our assumptions will have a 95\% probability that the parameter value is within our confidence interval. Claim two: the confidence interval that was produced from our data either does or does not contain the true parameter value, and we are 95\% confident that this interval contains the true parameter value.

\subsection{Z values}

Suppose we assumed a random sample generated an observed dataset $\mathcal{D}$
\begin{align}
    (X_{1},X_{2},\cdots,X_{n}) \\ 
    X_{i} \sim f
\end{align}

The following transformation
\begin{align}
    Z = \frac{X - \mathbb{E}(X)}{ \sqrt{V(X)} }
\end{align}
is called \textbf{standardizing} a random variable.

We can show that a standardized variable has an expected value of zero and variance of one using expected value and variance properties. 

\begin{align}
    \mathbb{E}(Z) &= \mathbb{E} \left( \frac{1}{V(X)} \left[X - \mathbb{E}(X)\right]  \right) \\ 
                  &=  \frac{1}{\sqrt{V(X)}} \left[\mathbb{E}(X) - \mathbb{E}(X) \right]  \\
                  &=0
\end{align}
The expected value of our standardized random variable $Z$ is zero.

\begin{align}
    V(Z) &= V \left( \frac{1}{\sqrt{V(X)}} \left[X - \mathbb{E}(X)\right]  \right) \\
         &=  \frac{1}{V(X)} \left[V(X) - 0\right] \\
         &=  1
\end{align}
The variance of our standardized random variable $Z$ is one.

In practice, we can transform our individual data points $x_{1},x_{2},\cdots,x_{n}$ into standard, or "z" scores by subtracting from each point $x_{i}$ the sample mean and sample standard deviation.

\ex Suppose we are given a dataset $\mathcal{D} = (1.05,4.20,-2.80,1.34)$. To transform these four points into four z-scores we need to follow three steps: (i) compute the sample mean, (ii) compute the sample standard deviation, and (iii) subtract from each point the sample mean and divide this difference by the sample standard deviation.

The sample mean is 
\begin{equation*}
    \overline{x} = (1.05+4.20-2.80+1.34)/4 = 0.95
\end{equation*}.

The sample standard deviation is
\begin{equation*}
    s = \left[(1.05-0.95)^2 + (4.20-0.95)^2 + (2.80-0.95)^2 + (1.34-0.95)^2\right]/4 = 2.49
\end{equation*}.

Our four z-scores are then 
\begin{equation*}
    z = ( (1.05-0.95)/2.49,(4.20-0.95)/2.49,(-2.80-0.95)/2.49,(1.34-0.95)/2.49)
\end{equation*}

Z-scores are useful for characterizing the probability of sample points in a dataset and for eliminating measurement units from a set of sample points. 
The z score for a sample point $z_{i}$ 
\begin{equation}
   z_{i} = \frac{x_{i} - \mathbb{E}(X) }{\sqrt{V(X)}}
\end{equation}
can be interpreted as the number of standard deviations the sample point $x_{i}$ is from the expected value of the random variable $X_i$.

Chebychev's inequality tells us that values generated by a random variable $X$ are likely close to the expected value, and so large z-scores are improbable while z-scores close to zero are probable.
A z-score is meant to map values to the probability that this observation occurred. Large z values should denote a small probability that this event occurred and small z values should denote a large probability that this event occurred.

Z-scores also eliminate measurement units $(u)$ from an observation by mapping $u$ to units of standard deviation. 
Mapping the units of two or more variables to the number of standard deviations from their mean is a convenient way to compare two variables (i) where one variable has large values and the second has small values (ii) where one variable has a large variance and the second has a small variance. 
Large, small values and large, small variances associated with a variable are often because of the chosen units. 




\subsection{Using the CLT to build a confidence interval}

The method of moments~(MoM) has given us an algorithm for estimating parameter values, however the MoM only provides a single estimate. 
Because we compute our estimated parameter values from a sample of data, we know that our estimate is not exact and will vary for every data set we collect.

A confidence interval for a parameter, or parameters, is a set of statistics---a lower bound $(L)$ and upper bound $(U)$---where we expect the true parameter values to lie with high probability. 
Confidence intervals take into account the size of our sample dataset, the variability in our estimate of parameter values, and should accompany a point estimate for a parameter.

\ex

More formally, given a random sample $(X_{1},X_{2},\cdots,X_{n})$,  a confidence interval for the parameter $\theta$ is a pair of statistics $[L(X), U(X)]$ such that $P(\theta \in {L(X),U(X)}) = 1-\alpha$. 
The value $\alpha$ is called the confidence coefficient and the interval $[L(X), U(X)]$ is called a $1-\alpha$ confidence interval.

One way to build a confidence interval is to use the Central Limit Theorem.
The CLT states that 
\begin{align}
    \overline{X} \sim \mathcal{N}\left( \mu, \sigma^{2} \right)
\end{align}
where $\mu$ is $\mathbb{E}(X)$ and $\sigma^{2}$ is $V(X)$. 

Suppose $(X_{1},X_{2},\cdots,X_{n})$

\begin{align}
    P\left( -z_{1-\alpha/2} < \frac{\hat{\theta} - \theta}{ \frac{\sigma}{\sqrt{n}}  } < z_{1-\alpha/2} \right) &= 1-\alpha \\ 
    P\left( -z_{1-\alpha/2} \frac{\sigma}{\sqrt{n}} < \hat{\theta} - \theta < z_{1-\alpha/2} \frac{\sigma}{\sqrt{n}} \right)&= 1-\alpha\\ 
    P\left( z_{1-\alpha/2} \frac{\sigma}{\sqrt{n}} > \theta - \hat{\theta} > -z_{1-\alpha/2} \frac{\sigma}{\sqrt{n}} \right) &= 1-\alpha\\ 
    P\left( \hat{\theta} + z_{1-\alpha/2} \frac{\sigma}{\sqrt{n}} > \theta   > \hat{\theta} -z_{1-\alpha/2} \frac{\sigma}{\sqrt{n}} \right)&= 1-\alpha \\ 
    P\left( \hat{\theta} - z_{1-\alpha/2} \frac{\sigma}{\sqrt{n}} < \theta   < \hat{\theta} +z_{1-\alpha/2} \frac{\sigma}{\sqrt{n}} \right) &= 1-\alpha\\
    P\left( \hat{\theta} - z_{1-\alpha/2} \frac{\hat{\sigma}}{\sqrt{n}} < \theta   < \hat{\theta} +z_{1-\alpha/2} \frac{\hat{\sigma}}{\sqrt{n}} \right) &= 1-\alpha
\end{align}
where $\sigma$ is equal to the standard deviation of $X$ or $\sigma = sd(X) = \sqrt{V(X)}$. 


\section{Exercises}

\begin{enumerate}
    \item Suppose we collect the following dataset $\mathval{D} = (10,2,6,7,17,3,1,1,6,5,4,1)$ and further we assume that each data point $d_{i}$ was generated by sampling from a sequence of i.i.d random variables  $d_{1} \sim X_{1}, d_{2} \sim X_{2}, \cdots, d_{12} \sim X_{12}$ where $X_{i} \sim \text{Geom}(p)$.  
    \begin{enumerate}
        \item State the LLN for the above problem
        \item Compute the sample mean of $\mathval{D}$
        \item What can we say about the sample mean, using the LLN, if we collect 13,14,$\cdots$ data points?  
    \end{enumerate}
    
    \item  Suppose we collect the following sample $\mathcal{D} = (10,2,6,7,17,3,1,1,6,5,4,1)$ and further we assume that each data point $d_{i}$ was generated by sampling from a sequence of i.i.d random variables  $d_{1} \sim X_{1}, d_{2} \sim X_{2}, \cdots, d_{12} \sim X_{12}$ where $X_{i} \sim \text{Pois}(\lambda)$.
    \begin{enumerate}
        \item State the LLN for the above problem
        \item Compute the sample variance
        \item What can we say about the sample variance, using the LLN, if we collect 13,14,$\cdots$ data points?
    \end{enumerate}
    
    \item Define independent and identically distributed random variables $B_{1},B_{2},B_{3},B_{4},B_{5} \sim \text{Geom}(p)$. Suppose we sample these rvs and collect $d = (12,9,1,5,3)$.
    \begin{enumerate}
        \item Use the method of moments to develop an estimator for $p$.
        \item Estimate $p$ from the above sample $\mathcal{D}$. 
        \item What is the estimated expected value? 
        \item What is the estimated variance? 
        \item What is the estimated $P(B_{1} = 1)$
    \end{enumerate}
    \item Assume $(Y_{1}, Y_{2},Y_{3}, \cdots, Y_{n})$ are a random sample where $Y_{i} \sim \mathcal{N}(\mu,\sigma^{2})$. Further assume we collected the data set $\mathcal{D} = (1.30, -2.11, 1.44, 0.35, -0.40, 0.61,-1.06, -0.86, -0.60, 0.19)$.
    \begin{enumerate}
        \item Use the method of moments to develop an estimator for $\mu$ and for $\sigma^{2}$.
        \item Estimate $\mu$ and $\sigma^{2}$ from the above sample $\mathcal{D}$. 
        \item What is the estimated expected value? 
        \item What is the estimated variance? 
        \item What is the estimated $P(Y_{1} = 2)$
    \end{enumerate}
    \item Lets assume we decide to study the incubation period for the influenza virus. The incubation period is defined as the number of days between when the virus infects a host and when that host becomes symptomatic. We collect from 5 individuals the date they came in contact with someone who was infected with influenza and the date they themselves were symptomatic.
    
    \begin{align}
        \mathcal{D} 
        = \begin{bmatrix}
            \text{Date of contact} & \text{Date of Symptoms}\\
            02/20  & 02/21\\
            04/12  & 04/17\\
            03/22  & 03/25\\
            10/24  & 10/26\\
            07/15  & 07/18\\
          \end{bmatrix}
    \end{align}
    
    \begin{enumerate}
        \item Provide a statistical setup for this data. Define a sample, assumptions about the sample, and a distribution for each random variable that is a part of the sample
        \item Estimate the parameters in your statistical setup using the Method of Moments
        \item Describe your results
    \end{enumerate}
    
    \item Suppose $(Y_{1},Y_{2},\cdots,Y_{n})$ is a random sample where $Y_{i} \sim \text{Pois}(\lambda)$.
    \begin{enumerate}
        \item Define an estimator for $\lambda$ using the MoM
        \item Characterize the distribution of $\hat{\lambda}$ using the CLT.
    \end{enumerate}
    
    \item The Aedes mosquito is a vector for yellow and dengue fever, the Zika virus, and Chikungunya. Tracking the daily incidence of Aedes mosquitoes in a given location is one signal associated with the incidence of these four diseases. Mosquitoes are routinely captured and counted and the daily frequency of mosquitoes is reported to departments of public health.
    
    Suppose we capture and count the number of mosquitoes in a specific county for two weeks, and compile this data in the following dataset $\mathcal{D} = (124,98,188,212,100,34,90,99,46,176,67,94,344,67)$ where one data point represents the number of mosquitoes collected in one day.
    
    \begin{enumerate}
        \item Define a set of random variables that we will use to model the daily frequency of mosquitoes. Include notation for the random sample and define a common Poisson distribution for all random variables.
        \item Use the method of moments to estimate parameter value $(\lambda)$ from the given dataset $\mathcal{D}$
        \item Estimate the probability that we observe 124 mosquitoes
        \item Estimate the probability that we observe 120 to 125 mosquitoes
        \item Estimate the expected value and the variance
        \item Let's rework the above model and assume that each random variable follows a Normal distribution. Use the method of moments to estimate parameters value $(\mu, \sigma^{2})$ from the given dataset $\mathcal{D}$.
        \item Estimate the expected value and the variance for the Normal model
    \end{enumerate}
    
    \item A Randomized Control trial~(RCT) is a common study design to compare the efficacy and safety of a novel device. Suppose the sponsor for a new cardiovascular device wishes to compare the safety of their device, which they define as the percent of patients who survive 7 days after the procedure, and the efficacy, which they define as the percent of patients who survive at one year after the procedure. \\ 
    
    The trial enrolls 50 patients to receive the device~(the device group) and 50 patients who receive optimal medical therapy~(the control group). The trial enrolls these 100 patients over the course of two years. At year 3 all 100 patients have been contacted at one year after their procedure~(called a patient's one year followup).\\
    
    We find that in the device group 8 patients did not survive at or after 7 days from the date of their procedure and 18 patients did not survive one year after the date of their procedure. In the control group 9 patients did not survive at or after 7 days and 33 patients did not survive one year after the procedure.
    
    \begin{enumerate}
        \item Define a set of random variables that we will use to model the number of patients in the device group who survive 7 days after the procedure out of a total of 50 patients and a second set of random variables to model the number of patients in the control group who survive 7 days after the procedure out of a total of 50 patients. Include mathematical notation for the random sample and a common distribution for both sets of random variables. 
        
        \item Use the Method of Moments to estimate the parameters for the distribution you chose for survival at or after 7 day for patients in the device group. Report the parameter estimate. 
        
        \item Use the Method of Moments to estimate the parameters for the distribution you chose for survival at or after 7 days for patients in the control group. Report the parameter estimate. 
        
        \item What can you conclude about the safety of the novel device compared to the safety of optimal medical therapy?  
        
        \item Use the Method of Moments to estimate the parameters for the distribution you chose for survival at one year for patients in the device group. Report the parameter estimate. 
        
        \item Use the Method of Moments to estimate the parameters for the distribution you chose for survival at one year for patients in the control group. Report the parameter estimate. 
        
        \item What can you conclude about the efficacy of the novel device compared to the efficacy of optimal medical therapy?   
        
    \end{enumerate}
    
    \item Suppose a random sample $(Y_{1},Y_{2},\cdots,Y_{10})$ generates the following dataset $\mathcal{D} = ( 0,-2.3,0.4,9.2,10,-9.8,3,3,3,0)$. Assume that $X_{i} \sim \mathcal{N}(\mu,\sigma^{2})$ and compute the corresponding ten z-scores. 
    
    \item Suppose we collect the following dataset $\mathcal{D} = ( 6.51, -1.11,  7.29,  0.23,  3.45,  0.85, -0.42,  5.66,  0.04, -1.31 )$. Further, lets assume $X_{i} \sim \mathcal{N}(\mu,\sigma^{2})$.
    \begin{enumerate}
        \item Please compute a $1-\alpha$ confidence interval for $\mu$. The symbol $z_{1-\alpha/2}$ should appear in this interval. 
        \item Please compute a 95\% confidence interval for $\mu$.
        \item Please compute a 80\% confidence interval for $\mu$.
        \item Why is your 95\% confidence interval larger than your 80\% confidence interval?
    \end{enumerate}
    
    \item Suppose we collect the following dataset $\mathcal{D} = (4, 0,  1, 2, 8, 13, 0, 1, 0, 3)$. Further, lets assume $X_{i} \sim \text{Geom}(p)$.
        \begin{enumerate}
            \item Please compute a $1-\alpha$ confidence interval for $\frac{1}{p}$. The symbol $z_{1-\alpha/2}$ should appear in this interval. 
            \item Please compute a 95\% confidence interval for $\frac{1}{p}$.
            \item Please compute a 80\% confidence interval for $\frac{1}{p}$.
        \end{enumerate}
        
    \item Suppose we collect the following dataset $\mathcal{D} = (0, 1, 1, 1, 1, 1, 1, 0, 1, 1)$. Further, lets assume $X_{i} \sim \text{Bernoulli}(\theta)$.
        \begin{enumerate}
            \item Please compute a $1-\alpha$ confidence interval for $\theta$. The symbol $z_{1-\alpha/2}$ should appear in this interval. 
            \item Please compute a 95\% confidence interval for $\theta$.
            \item Please compute a 80\% confidence interval for $\theta$.
        \end{enumerate}
        
    \item Suppose we collect the following dataset $\mathcal{D} = (6, 6, 2, 7, 3, 3, 5, 5, 7, 5)$. Further, lets assume $X_{i} \sim \text{Binomial}(50,\theta)$.
        \begin{enumerate}
            \item Please compute a $1-\alpha$ confidence interval for $\theta$. The symbol $z_{1-\alpha/2}$ should appear in this interval. 
            \item Please compute a 95\% confidence interval for $\theta$.
            \item Please compute a 80\% confidence interval for $\theta$.
        \end{enumerate}
        

    \item Suppose we collect the following dataset $\mathcal{D} = (1, 4, 2, 1, 1, 4, 3, 2, 3, 0)$. Further, lets assume $X_{i} \sim \text{Poisson}(\lambda)$.
        \begin{enumerate}
            \item Please compute a $1-\alpha$ confidence interval for $\lambda$. The symbol $z_{1-\alpha/2}$ should appear in this interval. 
            \item Please compute a 95\% confidence interval for $\lambda$.
            \item Please compute a 80\% confidence interval for $\lambda$.
        \end{enumerate}

\end{enumerate}