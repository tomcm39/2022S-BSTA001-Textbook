\chapterauthor{thomas mcandrew}{Lehigh University}
%\chapterauthor{Second Author}{Second Author Affiliation}
\chapter{Law of large numbers, Method of Moments, and the Central limit theorem}
\hspace{1mm}

\section{Introduction}\label{intro}

The law of large numbers~(LLN) and the central limit theorem underpin a large portion of statistical theory. 
We will see that the LLN will link proportions to probabilities, expected values to the sample mean, and allow us to estimate parameters of a random variable given a dataset.

The method of moments will be the primary tool we use to derive parameter estimates for parameters for a given distribution of a random variable. This method follows directly from the LLN.

Finally, we will explore the Central limit theorem which is the foundation of much of  generating hypothesis tests and building confidence intervals. 

\section{Independent and identically distributed and sample}

To model a dataset, $\mathcal{D}$, we will need to make assumptions about how our data set was generated.

A common assumption for a dataset with $n$ data points $\mathcal{D} = (x_{1}, x_{2}, \cdots, x_{n} )$ is that each data point was generated from a random variable $x_{1} \sim X_{1}, x_{2} \sim X_{2}, \cdots, x_{n} \sim X_{n}$. 
The dataset $\mathcal{D}$ is typically called a \textbf{sample}.

If we additionally assume that all the random variables are \textbf{identically distributed}, or that they have the same distribution $X_{1},X_{2}, \cdots, X_{n} \sim f$, and we assume that for any $i$ and $j$ that $X_{i}$ and $X_{j}$ are independent then we call our sample a \textbf{random sample}.

Because pairwise independence between random variables and that all random variables are identically distributed are two common assumptions, they are abbreviated as \textbf{i.i.d} which stands for "independent and identically distributed".

\section{Properties of the Expected value and Variance of a sum}

To explore the law of large numbers, we will need another property of the expectation.
The expected value of the sum of random variables is the sum of the expected value of each individual random variable. 
\begin{align}
    \mathbb{E}(X+Y) = \mathbb{E}(X) + \mathbb{E}(Y)
\end{align}

When we combine the properties of expected values we have already learned, we can state 
\begin{align}
    \mathbb{E}(aX+bY) = a \mathbb{E}(X) + b \mathbb{E}(Y)
\end{align}
and this is true for any number of random variables
\begin{align}
    \mathbb{E}\left( \sum_{i=1}^{N} a_{i} X_{i} \right) = a_{i} \sum_{i=1}^{N} \mathbb{E}(X_{i})
\end{align}

We start at the definition of the expected value to derive this property 
\begin{align}
    \mathbb{E}(X+Y) &= \sum_{x} \sum_{y} (X+Y) f_{X+Y}(x,y) \\ 
                    &=\sum_{x} \sum_{y} X f_{X+Y}(x,y) + Y f_{X+Y}(x,y) \\
                    &= \sum_{x} \sum_{y} X f_{X+Y}(x,y) +  \sum_{x} \sum_{y} Y f_{X+Y}(x,y) \\
                    &= \sum_{x} X f_{X}(x) + \sum_{y} Y f_{Y}(y) \\
                    &= \mathbb{E}(X) + \mathbb{E}(Y)
\end{align}

\ex Suppose $X \sim \text{Geom}(1/4)$ and $Y \sim \text{Bernoulli}(1/2)$ then $\mathbb{E}(X+Y) = \frac{1}{1/4} + \frac{1}{2} = 4 \frac{1}{2}$. 

The variance is an expected value and so it follows that 
\begin{align}
    V(X+Y) = V(X) + V(Y)
\end{align}
and we can state more generally that for two independent random variables 
\begin{align}
    V(aX+bY) = a^{2}V(X) + b^{2}V(Y)
\end{align}

\ex Let $X \sim \text{Binomial}(N,\theta)$ and $Y \sim \text{Binomial}(M,\gamma)$ then $V(X-Y) = V(X + (-1) Y) = V(X) + (-1)^{2} V(Y) = N \theta (1-\theta) + M \gamma (1-\gamma)$.

\section{Law of large numbers}\label{intro}

The intuition behind the law of large numbers can be characterized by the following experiment: you are asked to flip a fair coin and record the whether the coin is heads up or tails up. After 10 flips you are asked to compute the proportion of heads up flips, after 50 flips you are asked to compute this proportion, after 100, 1,000, 10,000 flips you are asked to compute the proportion of heads up flips. We expect that if this coin is fair that the proportion of flips with heads face up will get closer and closer to 0.50. 

The law of large numbers attempts to describe this intuition.

Define a sequence of random variables $X_{1}, X_{2}, \cdots, X_{n}$ such that any pair of random variables, $X_{i}$ and $X_{j}$ are independent (that is $P(X_{i} = x | X_{j} = y) =P(X_{i} = x)$. Finally, transform this sequence into a single random variable $\overline{X}$ that is equal to $\overline{X} = \dfrac{X_{1} + X_{2} + X_{3} + \cdots X_{N}}{N}$. 

Then the law of large numbers~(LLN) states that given any small number $\epsilon$ that is greater than 0 as $n$ grows towards infinity~$(n \to \infty)$
\begin{align}
    P( | \overline{X}_{n} - \mu | > \epsilon ) \to 0
\end{align}
where $\mu = \mathbb{E}(\overline{X})$.

We can picture a distribution $Z_{n} = | \overline{X}_{n} - \mu |$ that depends on $n$ and as $n$ increase the random variable $Z_{n}$ assigns more and more probability to the value 0.
<pic>

The LLN has many applications. 

\begin{VT1}
\VH{Proportions approximate probabilities}
Let a sequence of Bernoulli distributed random variables be (i) pairwise independent and (ii) be distributed the same $X_{1}, X_{2}, \cdots, X_{N} \sim \text{Bern}(\theta)$.

Lets take a look at the transformation $\overline{X}$.
Consider a sequence of two random variables with a Bernoulli distribution. Then define
\begin{align}
    S = X_{1} + X_{2}
\end{align}

The expected value of $S$ is 

\begin{align}
    \mathbb{E}(S) &= \mathbb{E}(X_{1} + X_{2}) \\
                  &= \mathbb{E}(X_{1}) + \mathbb{E}(X_{2})\\
                  &= \theta + \theta \\ 
                  &= 2\theta
\end{align}
and the expected value of $\overline{X}$ is then 
\begin{align}
    \mathbb{E}(\overline{X}) = \frac{\mathbb{E}(S)}{2}  = \frac{2 \theta}{2} = \theta
\end{align}

To gain intuition about what a sample from $\overline{X}$ looks like, assume we collect data points from $n$ random variables $x_{1} \sim X_{1}, x_{2} \sim X_{2}, \cdots, x_{n} \sim X_{n}$.
An example of a single data point $d$ could be
\begin{align}
    d = (0,1,0,1,0,0,1,1,1,0)
\end{align}
and the sample mean $\overline{x}$, one sample from $\overline{X}$, is then 
\begin{align}
    S &= \sum_{i=1}^{10} d_{i} \\
    \overline{x} &= \frac{X}{n}
\end{align}

However, the value of $S$ can be counted in two different ways. 
We can sum up $d_{i}$ one by one in order, or we can multiply zero by the frequency of zeros in our dataset, multiply 1 by the frequency of ones in our dataset, and sum both of these products.
\begin{align}
    S &= \sum_{i=1}^{10} d_{i} \\
      &= 0+1+0+1+0+0+1+1+1+0\\
    S &= N(0) \cdot 0 + N(1) \cdot 1
\end{align}
where $N(x)$ is the number of values $x$ in our dataset.

If we use the above method to sum our data points the sample mean simplifies to  
\begin{align}
    \overline{x} &= \frac{ N(0) \cdot 0 + N(1) \cdot 1  }{N} \\ 
                 &= \frac{N(1)}{N}.
\end{align}
The sample mean is the proportion of 1s $(p_{n})$ from our sample of $n$ data points, and the law of large number then says that 
\begin{align}
    &P( | \overline{X} - \mu | > \epsilon ) \to 0 \\ 
    &P( | p_{n} - \theta | > \epsilon ) \to 0
\end{align}
or, roughly, that the sample proportion approaches $\theta$, the true probability that a $1$ will appear.

\end{VT1}


\begin{VT1}
\VH{The Relationship between the expected value and the sample mean}

Define a discrete random variable $X_{1}, X_{2}, \cdots, X_{n} \sim f_{X}$, $supp(X_{i}) = \{1,2,3\}$

Lets assume $n=10$ and take a closer look at $S$.
One sample from our $n=10$\ random variables could be  $d = (2,1,3,2,3,2,3,1,2,2)$ and we can compute the sum $S = 2+1+3+2+3+2+3+1+2+2$. 
However, we could have also summed these ten numbers by multiplying each unique number in the dataset by the frequency this number occurs and then summing these products: $S = N(1) \cdot 1 + N(2) \cdot 2 + N(3) \cdot 3$, where $N(x)$ is the number of times the value $x$ appears.
For our specific example $S = 2 \cdot 1 + 5 \cdot 2 + 3 \cdot 3$.
The sample mean is then 
\begin{align}
    \overline{x} &= \frac{S}{N} \\ 
                 &= \frac{N(1) \cdot 1 + N(2) \cdot 2 + N(3) \cdot 3}{N}\\
                 &= \frac{N(1)}{N} 1 + \frac{N(2)}{N} 2  + \frac{N(3)}{N} 3 
\end{align}
We know from our discussion of the LLN and Bernoulli distributed random variables that $\frac{N(1)}{N}$ is the proportion of $1$s , $p_{n}(1)$, and this proportion will approach the true probability of a 1 as we collect more data (as $n \to \infty$).

Let's rewrite the above sample mean computation
\begin{align}
    \overline{x} &= \frac{N(1)}{N} 1 + \frac{N(2)}{N} 2  + \frac{N(3)}{N} 3 \\ 
                 &= p_{n}(1) 1 + p_{n}(2) 2  + p_{n}(3) 3 \\ 
\end{align}
As $n \to \infty$
\begin{align}
    \overline{x} &= \frac{N(1)}{N} 1 + \frac{N(2)}{N} 2  + \frac{N(3)}{N} 3 \\ 
                 &= f_{X}(1) 1 + f_{X}(2) 2  + f_{X}(3) 3
\end{align}
but the above is the expected value of $X$, $\mathbb{E}(X)$, and so as $n$ gets larger the sample mean will become closer to the true expected value.
\end{VT1}


\subsection{LLN for transformations}

Earlier we saw that by definition if we define a random variable $X$ and a function $g$ then
\begin{align}
    \mathbb{E}\left[ g(X) \right] = \sum_{i \in supp(X)} g(x_{i}) f_{X}(x_{i})
\end{align}
Because the above is equivalent to the expectation of a transformed random variable $Y = g(X)$ the LLN has a similar result for any transformation of a random variable. 

As $n \to \infty$,
\begin{align}
    P( | \overline{Y}_{n} - \mu_{Y}| > \epsilon ) \to 0 
\end{align}
where $\mu_{Y} = \mathbb{E}(Y) = \mathbb{E}\left[g(X)\right]$.

The above implies that the LLN applies to any transformation of a random variable, including 
\begin{align}
    S = \frac{\sum_{i=1}^{N} ( X_{i} - \mu  )^{2}}{N}
\end{align}
where $\mu = \mathbb{E}[g(X)]$. 

Let us find a simple expression for $\mathbb{E}(S)$. 
\begin{align}
    \mathbb{E}(S) &= \mathbb{E} \left[ \frac{\sum_{i=1}^{N} ( X_{i} - \mu  )^{2}}{N}\right] \\ 
                  &= \frac{1}{N} \sum_{i=1}^{N} \mathbb{E}\left[( X_{i} - \mu  )^{2}\right]\\
                  &= \frac{1}{N} \sum_{i=1}^{N} V(X)\\
                  &= \frac{1}{N}  N V(X)  = V(X)
\end{align}

Because $\mathbb{E}(S) = V(X)$, the LLN states that for any $\epsilon >0$ as $n \to \infty$
\begin{align}
    P(|S - V(X)| > \epsilon) \to 0
\end{align}
or that $S$ approaches the true variance of $X$.


\section{Statistics, Estimators, and the Method of Moments}\label{intro}

Given a random sample from $X_{1}, X_{2}, X_{3}, \cdots, X_{n}$, a  \textbf{statistic} is a 




\section{Central limit theorem}\label{intro}


\section{Exercises}

\begin{enumerate}
    \item Suppose we collect the following sample $d = (10,2,6,7,17,3,1,1,6,5,4,1)$ and further we assume that each data point $d_{i}$ was generated by sampling from a sequence of i.i.d random variables  $d_{1} \sim X_{1}, d_{2} \sim X_{2}, \cdots, d_{12} \sim X_{12}$ where $X_{i} \sim \text{Geom}(p)$.  
    \begin{enumerate}
        \item State the LLN for the above problem
        \item Compute the sample mean of $d$
        \item What can we say about the sample mean, using the LLN, if we collect 13,14,$\cdots$ data points?  
    \end{enumerate}
    
    \item  Suppose we collect the following sample $d = (10,2,6,7,17,3,1,1,6,5,4,1)$ and further we assume that each data point $d_{i}$ was generated by sampling from a sequence of i.i.d random variables  $d_{1} \sim X_{1}, d_{2} \sim X_{2}, \cdots, d_{12} \sim X_{12}$ where $X_{i} \sim \text{Pois}(\lambda)$.
    \begin{enumerate}
        \item State the LLN for the above problem
        \item Compute the sample variance
        \item What can we say about the sample variance, using the LLN, if we collect 13,14,$\cdots$ data points?
    \end{enumerate}
    
    \item Define independent and identically distributed random variables $B_{1},B_{2},B_{3},B_{4},B_{5} \sim \text{Geom}(p)$. Suppose we sample these rvs and collect $d = (12,9,1,5,3)$.
    \begin{enumerate}
        \item Use the method of moments to develop an estimator for $p$.
        \item Estimate $p$ from the above sample $d$. 
        \item What is the estimated expected value? 
        \item What is the estimated variance? 
        \item What is the estimated $P(B_{1} = 1)$
    \end{enumerate}
    \item
    \item
\end{enumerate}