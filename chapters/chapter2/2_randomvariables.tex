\chapterauthor{thomas mcandrew}{Lehigh University}
%\chapterauthor{Second Author}{Second Author Affiliation}
\chapter{Random variables}
\hspace{1mm}

\section{Introduction}\label{intro}

The foundations of probability are built on sets,
yet data is more naturally stored and more easily computed on if it is represented numerically.  

Random variables match each outcome in our sample space to a value on the number line. 

In addition to computational advantages, random variables help us extract from our data the most important characteristics, and they serve as building blocks which we can use to create powerful models.
Random variables are also a language we can use to communicate our modeling efforts to other mathematicians, statisticians, and data scientists.  

Suppose we hypothesize that the frequency of social media posts on some popular outlet are related to influenza-like illness~(ILI)---a syndromic diagnosis suggesting a patient may have influenza. A patient is diagnosed with ILI if their temperature is measured to be at or above 38C and symptoms resembling the flu.
Because influenza is most active in winter and spring, we collect a random sample, each day, of social media posts from September to May and in addition we collect the proportion of patients who are admitted to the hospital and are diagnosed with influenza-like illness at the US national level. 

The above hypothesis, data collection, and future inference has numerous details. 
However, we will see shortly that we can simplify our hypothesis by using random variables. 

\section{Maps from the sample space to the number line}\label{maps}

Given a sample space $\mathcal{G}$, a \textbf{random variable}, (e.g. $X$), is a function from each element in $\mathcal{G}$---from each outcome---to a value on the real number line.
The real number line contains all numbers: integer and decimal, from negative to positive infinity.

\textbf{Example:} Suppose our sample space contains two elements $\mathcal{G} = \{ a,b \}$. We may decide to define a random variable $X$ that maps the outcome $a$ to the value $-1$ and the outcome $b$ to the value $1$. In otherwords, $X(a)=-1$ and $X(b)=1$. We could as well define a random variable $Y$ on the same sample space such that $Y(a)=0$ and $Y(b)=1$.

\textbf{Example:} Suppose our sample space contains all integers from 0 to 1000 $\mathcal{G} = \{0,1,2,3\cdots,1000 \}$. We may be most interested in when an integer is even or odd, and so we can define a random variable $Y(y)=0$ when $y$, our outcome, is an odd integer and $Y(y)=1$ when $y$ is even. This is an example of how a random variable can distill down a sample space with many outcomes into a random variable with two.

\textbf{Example:} Suppose we decide to study the relationship between the cumulative total number of cigarettes smoked by a person form the date that they started smoking and the presence of lung cancer.
We define our sample space to be $\mathcal{G} = \{ (x,y) | x \in \mathbb{Z}, y \in \{0,1\} \}$. We define two random variables, a random variable $X$ that maps the outcome $(x,y)$ to the value in the first position $x$, and a random variable $Y$ that maps the $outcome (x,y)$ to the value in the second position $y$. Though our outcomes are linked, we can use random variables to think about two separate outcomes---cigarettes smoked and lung cancer---and how they interact.

\section{A new sample space}\label{intro}

When we build a random variable $(X)$ that maps outcomes to values on the number line we create a new sample space which we will call the support of $X$ or $supp(X)$. 
Define a sample space $\mathcal{G}$ without outcomes $o_{i}$.
Then the \textbf{support of X} is
\begin{align}
    supp(X) = \{x | X(o) = x \text{ for some outcome } o \text{ in } \mathcal{G} \}
\end{align}
Our new sample space is the set of all the potential values that our random variable $X$ can produce.
This is a sample space linked to $\mathcal{G}$, but in practice after we develop a random variable we often no longer reference $\mathcal{G}$.

\ex In our above example where $\mathcal{G} = \{a,b\}$, the random variable $X$ has support $supp(X) = \{-1,1\}$ and $supp(Y)=\{0,1\}$.
Lets look at another example, when above $\mathcal{G}$ is  the set of all integers from 0 to 1000. 
Even though the sample space is quite large, the random variable that maps the integers to 0 when they are odd and 1 when even has a small support $(supp(Y) = \{0,1\})$.   

\section{How to assign probabilities to a random variable}

Random variable themselves do not require that we include the probability of each of their values. Random variables are a function from outcomes to the real numbers---nothing more. 
That said, in practice we build random variables expecting that the probabilities we assign to outcomes in our sample space will correspond to probabilities assigned to values of our random variable.

We assign a probability to the value $x$, which belongs in the support of random variable $X$, the sum of the probabilities of all the outcomes that $X$ maps to $x$.
\begin{equation}
    P(X=x) = P(o_{1}) + P(o_{2}) + \cdots + P(o_{n})    
\end{equation}
where each outcome $o_{1},o_{2},\cdots,o_{n}$ is mapped by $X$ to the value $x$. In other words, $X(o)=x$ for each of $o_{1},o_{2},\cdots,o_{n}$.

\ex Define a $\samplespace = \{a,b,c,d,e\}$ and a random variable $X$ that maps the outcomes to the following values
\begin{table}[ht!]
    \centering
    \begin{tabular}{ccc}
        Outcome & P(outcome) & X(outcome)  \\
        \hline
        a & 0.1  & 0\\
        b & 0.25 & 1\\
        c & 0.15 & 1\\
        d & 0.3  & 2\\
        e & 0.2  & 0\\
    \end{tabular}
\end{table}

We assign the probability that $X=0$ as the sum of the probabilities assigned to outcome $a$ and outcome $e$, or
\begin{align}
    P(X=0) &= P(\{a\}) + P(\{e\})\\
           &= 0.1+0.2 = 0.3
\end{align}
We can run the same procedure for all the elements in the support of $X$, 
\begin{align}
    P(X=1) &= P(\{b\}) + P(\{c\})\\
           &= 0.25+0.15 = 0.40\\
    P(X=2) &= P(\{d\}) = 0.3 = 0.30,
\end{align}
and organize our work in a table
\begin{table}[ht!]
    \centering
    \begin{tabular}{c|c}
        X & P(X=x) \\
        \hline
        0 & 0.30\\
        1 & 0.40\\
        2 & 0.30
    \end{tabular}
\end{table}

A \textbf{probability distribution} for a random variable $X$ is a set of tuples where the first position in each tuple is a value in the support of $X$ and the second position in the tuple is the corresponding probability assigned to that value. 

\ex A probability distribution for the random variable $X$ above is $\{(0,0.30),(1,0.40),(2,0.30)\}$.

\ex Imagine we run an experiment that collects data on marathon runners. We decide to collect the number of elapsed minutes until they finish the race. Our sample space is defined as all positive integers $\samplespace = \{1,2,3,\cdots,\}$. We may decide to build a random variable $X$ that maps outcomes less than 60 to the value 1, outcomes from 61 to 120 to the value 2, and outcomes greater than 120 to the value 3. One potential probability distribution for $X$ is $\{(1,0.10),(2,0.50),(3,0.40)\}$. For this probability distribution, $P(X=1) = 0.10$, $P(X=2) = 0.50$, and $P(X=3) = 0.40$.

\begin{figure}[ht!]
    \includegraphics{chapters/chapter2/rvsLink.pdf}
    \caption[List]{A sample space $\samplespace$ with elements $\{a,b,c,d,e\}$ and a random variable $X$ that maps each element in $\samplespace$ to one the values: 0, 1, or 2. Probabilities corresponding to outcomes in the sample space and how they map to probabilities for each value of $X$ are shown in blue.}
\end{figure}

\section{Probability mass function}

There are several supportive tools that we can use to help us better understand random variable we create.
The first is the probability mass function, or p.m.f.
The \textbf{probability mass function} is a \underline{function} that maps values in the support of a random variable $X$ to their corresponding probabilities.
Inputs are values of $X$, outputs are probabilities.

The probability mass function is a convenient way to organize a probability distribution and it allows us to transfer all the information we know about functions to random variables.

\ex Define a random variable $Y$ with support $\{-1,0,1\}$, probability distribution \{(-1,0.2),(0,0.5),(1,0.3)\}, and probability mass function 
\begin{equation}
    f(y) = \begin{cases}
            0.2 & \text{ when } y=-1\\
            0.5 & \text{ when } y = 0\\
            0.3 & \text{ when } y=1
            \end{cases}
\end{equation}
The function---our probability mass function---is a type of function called a \textbf{piecewise} function.

We can ask our pm.f. to return the probability for a given value
\begin{equation}
    f(1) = 0.3
\end{equation}
and we can visualize our probability mass function using, for example, a barplot.



\section{Cumulative mass function}

The \textbf{cumulative mass function} is a \underline{function} that maps values in the support of a random variable $X$ to their corresponding probabilities.


\section{Two or more random variables}

\section{A Model}
