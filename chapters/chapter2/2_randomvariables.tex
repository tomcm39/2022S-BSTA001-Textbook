\chapterauthor{thomas mcandrew}{Lehigh University}
%\chapterauthor{Second Author}{Second Author Affiliation}
\chapter{Random variables}
\hspace{1mm}

\section{Introduction}\label{intro}

The foundations of probability are built on sets,
yet data is more naturally stored and more easily computed on if it is represented numerically.  

Random variables match each outcome in our sample space to a value on the number line. 

In addition to computational advantages, random variables help us extract from our data the most important characteristics, and they serve as building blocks which we can use to create powerful models.
Random variables are also a language we can use to communicate our modeling efforts to other mathematicians, statisticians, and data scientists.  

Suppose we hypothesize that the frequency of social media posts on some popular outlet are related to influenza-like illness~(ILI)---a syndromic diagnosis suggesting a patient may have influenza. A patient is diagnosed with ILI if their temperature is measured to be at or above 38C and symptoms resembling the flu.
Because influenza is most active in winter and spring, we collect a random sample, each day, of social media posts from September to May and in addition we collect the proportion of patients who are admitted to the hospital and are diagnosed with influenza-like illness at the US national level. 

The above hypothesis, data collection, and future inference has numerous details. 
However, we will see shortly that we can simplify our hypothesis by using random variables. 

\section{Maps from the sample space to the number line}\label{maps}

Given a sample space $\mathcal{G}$, a \textbf{random variable}, (e.g. $X$), is a function from each element in $\mathcal{G}$---from each outcome---to a value on the real number line.
The real number line contains all numbers: integer and decimal, from negative to positive infinity.

\textbf{Example:} Suppose our sample space contains two elements $\mathcal{G} = \{ a,b \}$. We may decide to define a random variable $X$ that maps the outcome $a$ to the value $-1$ and the outcome $b$ to the value $1$. In otherwords, $X(a)=-1$ and $X(b)=1$. We could as well define a random variable $Y$ on the same sample space such that $Y(a)=0$ and $Y(b)=1$.

\textbf{Example:} Suppose our sample space contains all integers from 0 to 1000 $\mathcal{G} = \{0,1,2,3\cdots,1000 \}$. We may be most interested in when an integer is even or odd, and so we can define a random variable $Y(y)=0$ when $y$, our outcome, is an odd integer and $Y(y)=1$ when $y$ is even. This is an example of how a random variable can distill down a sample space with many outcomes into a random variable with two.

\textbf{Example:} Suppose we decide to study the relationship between the cumulative total number of cigarettes smoked by a person form the date that they started smoking and the presence of lung cancer.
We define our sample space to be $\mathcal{G} = \{ (x,y) | x \in \mathbb{Z}, y \in \{0,1\} \}$. We define two random variables, a random variable $X$ that maps the outcome $(x,y)$ to the value in the first position $x$, and a random variable $Y$ that maps the $outcome (x,y)$ to the value in the second position $y$. Though our outcomes are linked, we can use random variables to think about two separate outcomes---cigarettes smoked and lung cancer---and how they interact.

\section{A new sample space}\label{intro}

When we build a random variable $(X)$ that maps outcomes to values on the number line we create a new sample space which we will call the support of $X$ or $supp(X)$. 
Define a sample space $\mathcal{G}$ without outcomes $o_{i}$.
Then the \textbf{support of X} is
\begin{align}
    supp(X) = \{x | X(o) = x \text{ for some outcome } o \text{ in } \mathcal{G} \}
\end{align}
Our new sample space is the set of all the potential values that our random variable $X$ can produce.
This is a sample space linked to $\mathcal{G}$, but in practice after we develop a random variable we often no longer reference $\mathcal{G}$.

\ex In our above example where $\mathcal{G} = \{a,b\}$, the random variable $X$ has support $supp(X) = \{-1,1\}$ and $supp(Y)=\{0,1\}$.
Lets look at another example, when above $\mathcal{G}$ is  the set of all integers from 0 to 1000. 
Even though the sample space is quite large, the random variable that maps the integers to 0 when they are odd and 1 when even has a small support $(supp(Y) = \{0,1\})$.   

\section{How to assign probabilities to a random variable}

Random variable themselves do not require that we include the probability of each of their values. Random variables are a function from outcomes to the real numbers---nothing more. 
That said, in practice we build random variables expecting that the probabilities we assign to outcomes in our sample space will correspond to probabilities assigned to values of our random variable.

We assign a probability to the value $x$, which belongs in the support of random variable $X$, the sum of the probabilities of all the outcomes that $X$ maps to $x$.
\begin{equation}
    P(X=x) = P(o_{1}) + P(o_{2}) + \cdots + P(o_{n})    
\end{equation}
where each outcome $o_{1},o_{2},\cdots,o_{n}$ is mapped by $X$ to the value $x$. In other words, $X(o)=x$ for each of $o_{1},o_{2},\cdots,o_{n}$.

\ex Define a $\samplespace = \{a,b,c,d,e\}$ and a random variable $X$ that maps the outcomes to the following values
\begin{table}[ht!]
    \centering
    \begin{tabular}{ccc}
        Outcome & P(outcome) & X(outcome)  \\
        \hline
        a & 0.1  & 0\\
        b & 0.25 & 1\\
        c & 0.15 & 1\\
        d & 0.3  & 2\\
        e & 0.2  & 0\\
    \end{tabular}
\end{table}

We assign the probability that $X=0$ as the sum of the probabilities assigned to outcome $a$ and outcome $e$, or
\begin{align}
    P(X=0) &= P(\{a\}) + P(\{e\})\\
           &= 0.1+0.2 = 0.3
\end{align}
We can run the same procedure for all the elements in the support of $X$, 
\begin{align}
    P(X=1) &= P(\{b\}) + P(\{c\})\\
           &= 0.25+0.15 = 0.40\\
    P(X=2) &= P(\{d\}) = 0.3 = 0.30,
\end{align}
and organize our work in a table
\begin{table}[ht!]
    \centering
    \begin{tabular}{c|c}
        X & P(X=x) \\
        \hline
        0 & 0.30\\
        1 & 0.40\\
        2 & 0.30
    \end{tabular}
\end{table}

A \textbf{probability distribution} for a random variable $X$ is a set of tuples where the first position in each tuple is a value in the support of $X$ and the second position in the tuple is the corresponding probability assigned to that value. 

\ex A probability distribution for the random variable $X$ above is $\{(0,0.30),(1,0.40),(2,0.30)\}$.

\ex Imagine we run an experiment that collects data on marathon runners. We decide to collect the number of elapsed minutes until they finish the race. Our sample space is defined as all positive integers $\samplespace = \{1,2,3,\cdots,\}$. We may decide to build a random variable $X$ that maps outcomes less than 60 to the value 1, outcomes from 61 to 120 to the value 2, and outcomes greater than 120 to the value 3. One potential probability distribution for $X$ is $\{(1,0.10),(2,0.50),(3,0.40)\}$. For this probability distribution, $P(X=1) = 0.10$, $P(X=2) = 0.50$, and $P(X=3) = 0.40$.

\begin{figure}[ht!]
    \includegraphics{chapters/chapter2/rvsLink.pdf}
    \caption[List]{A sample space $\samplespace$ with elements $\{a,b,c,d,e\}$ and a random variable $X$ that maps each element in $\samplespace$ to one the values: 0, 1, or 2. Probabilities corresponding to outcomes in the sample space and how they map to probabilities for each value of $X$ are shown in blue.}
\end{figure}

\section{Probability mass function}

There are several supportive tools that we can use to help us better understand random variable we create.
The first is the probability mass function, or p.m.f.
The \textbf{probability mass function} is a \underline{function} that maps values in the support of a random variable $X$ to their corresponding probabilities.
Inputs are values of $X$, outputs are probabilities.

The probability mass function is a convenient way to organize a probability distribution and it allows us to transfer all the information we know about functions to random variables.

\ex Define a random variable $Y$ with support $\{-1,0,1\}$, probability distribution \{(-1,0.2),(0,0.5),(1,0.3)\}, and probability mass function 
\begin{equation}
    f(y) = \begin{cases}
            0.2 & \text{ when } y=-1\\
            0.5 & \text{ when } y = 0\\
            0.3 & \text{ when } y=1
            \end{cases}
\end{equation}
The function---our probability mass function---is a type of function called a \textbf{piecewise} function.

We can ask our pm.f. to return the probability for a given value
\begin{equation}
    f(1) = 0.3
\end{equation}
and we can visualize our probability mass function using, for example, a barplot.
\begin{figure}[ht!]
    \centering
    \includegraphics{chapters/chapter2/pmfviz.pdf}
    \caption{A barplot for visualizing the probability mass function of our random variable $Y$. The support of $Y$ is plotted on the horizontal axis and height of each bar corresponds to the probability assigned to that value in the support.\label{fig.pmfviz}}
\end{figure}

\textbf{Distributed as $f$:} Because we can use the probability mass function to describe the probability distribution of a random variable, we will often write 
\begin{equation}
    Y \sim f
\end{equation}
The above formula is read "the random variable $Y$ is distributed as $f$", and what we mean when a random variable is distributed as $f$ is that the support of $Y$ is the same as the domain of the function $f$ and that the probability of a value $y$ is equal to $f(y)$, or 
\begin{align}
    supp(Y) &= dom(f)\\
    P(Y = y) &= f(y)
\end{align}

The probability mass function is a convenient method for assigning probabilities to random variables and visualizing the distribution of a random variable.

\section{Cumulative mass function}

The \textbf{cumulative mass function} is a \underline{function} that maps values in the support of a random variable $X$ to the probability that the random variable is less than or equal to this value, or $P(X \leq x)$.

We use a capital $F$ to denote a cumulative mass function (c.m.f).
The c.m.f. corresponding to random variable $X$ has a domain equal to the support of $X$ and produces values between 0 and 1 (the values a function can produce is called the function's \textbf{image}). 

\begin{align}
    supp(X) &= dom(F)\\
    image(F) &= [0,1]\\
    P(X \leq x) &= F(x)
\end{align}

The c.m.f. too can be visualized and we could also use the c.m.f. to describe the probability distribution of a random variable. 
This is because we can use the c.m.f. to derive the p.m.f.

\ex For a random variable 
\begin{align}
    X &\sim f\\
    supp(X) &= \{0,1,2,3\}\\
    f(x) &= \begin{cases}
     0 & 0.1\\
     1 & 0.3\\
     2 & 0.2\\
     3 & 0.4\\
    \end{cases}
\end{align}
The c.m.f is then 
\begin{align}
    F(x) =  
    \begin{cases}
     0 & 0.1\\
     1 & 0.3 + 0.1 = 0.4\\
     2 & 0.2+0.3+0.1 = 0.6\\
     3 & 0.4+0.2+0.3+0.1 = 1\\
     \end{cases}
\end{align}

We can use the c.m.f $(F(x))$ to compute any p.m.f ($f(x)$) too by noticing that for a support $\{x_{0},x_{1},x_{2},x_{3}, \cdots,x_{n-1}\cdots,x_{n}\}$ where the values in this set are ordered form smallest to largest 
\begin{align}
    f(x_{i}) &= \left[f(x_{i}) + f(x_{i-1}) + \cdots f(x_{0}) \right] - \left[f(x_{i-1}) + \cdots f(x_{0})\right] \\ 
    &=F(x_{i}) - F(x_{i-1}).
\end{align}
Because the p.d.f. and c.m.f equivalently describe the probability distribution of a random variable we can write $X\sim F$ or $X \sim f$.  

\section{Two or more random variables}

A single random variable is a useful way to describe a sample space and the probabilities assigned to values on the number line corresponding to outcomes.

But more complicated scientific questions often require several random variables and rules for how they interact.
We will explore how to generate multiple random variables  on the same sample space. 
Then we will develop an approach to define probabilities for combinations of random variables, a joint probability mass function and joint cumulative mass function.

Suppose we design a sample space $\samplespace$ and build two random variables on this space $X$ and $Y$.
Because $X$ and $Y$ are random variables we can think of the support of $X$ and support of $Y$ as two new sample spaces. Lets assume that the outcomes in $supp(X) = \{x_{1},x_{2},x_{3},\cdots,x_{N}\}$ and that the outcomes in $supp(Y) = \{y_{1},y_{2},\cdots,y_{M}\}$. 
Then we can define a new set of outcomes in the space $supp(X) \times supp(Y) = \{x_{i}, y_{j}\}$ for all combinations of $i$ from $1$ to $N$ and $j$ from $1$ to $M$.

This new space above maps outcomes in $\samplespace$ to a tuple $(x_{i},y_{j})$  where the outcomes were mapped by $X$ to the value $x_{i}$ and by $Y$ to the value $y_{j}$. 
If we assign the probability of $(x_{i},y_{j})$ to be the the sum of the probabilities of all outcomes in $\samplespace$ that $X$ maps to $x_{i}$ and $Y$ maps to $y_{i}$ then we call this a \textbf{joint probability distribution}. 






\section{A Model}
