\documentclass[krantz1,ChapterTOCs]{krantz}
\usepackage{fixltx2e,fix-cm}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{makeidx}
\usepackage{multicol}
\usepackage{hyperref}
\usepackage{xcolor}

\begin{document}

\begin{enumerate}
    \item Suppose that we collected a dataset with a single observation $\mathcal{D} = \{ 12 \}$. Please compute the probability of $\mathcal{D}$ if we assume our data was generated by a random variable $X$
    \begin{enumerate}
        \item Such that $X \sim \text{Geom}(p)$. Assume we know the parameter value $p$.
        
        \begin{enumerate}
            \item {\color{red} 
                \begin{align}
                    P(\mathcal{D} | p) = P(12 | p) =  p (1-p)^{12-1}
                \end{align}
            }
        \end{enumerate}
        
        \item Such that $X \sim \text{Pois}(\lambda)$. Assume we know the parameter value $\lambda$. 
        
        \begin{enumerate}
            \item {\color{red} 
                \begin{align}
                    P(\mathcal{D} | p) = P(12 | \lambda) = \frac{e^{-\lambda} \lambda^{12}}{12!}
                \end{align}
            }
        \end{enumerate}
 
        \item Such that $X \sim \mathcal{N}(\mu,\sigma^{2})$.
        Assume we know the parameter values $(\mu, \sigma^{2})$.
        
        \begin{enumerate}
            \item {\color{red}
            \begin{align*}
                    P(\mathcal{D} | \mu, \sigma^{2}) = P(12 | \mu, \sigma^{2})\\
                    &= \frac{1}{\sqrt{2 \pi \sigma^{2}}} \exp\left\{ -\frac{1}{2}\left( 12 - \mu\right)^{2}  / \sigma^{2}\right\}
                \end{align*}
            }
        \end{enumerate}
        
    \end{enumerate}
    
    \item Suppose that we collected a dataset with a two observations $\mathcal{D} = \{ 12, 2 \}$. Please compute the likelihood function for $\mathcal{D}$ if we assume our data was generated by a random variable $X$
        \begin{enumerate}
        \item Such that $X \sim \text{Geom}(p)$. Assume we know the parameter value $p$.
        
        \begin{enumerate}
            \item {\color{red} 
                \begin{align*}
                    P(\mathcal{D} | p) = P(12, 2 | p) &=  p (1-p)^{12-1} \cdot  p (1-p)^{2-1} \\ 
                    &= p^{2} (1-p)^{(12-1) + (2-1)}
                \end{align*}
            }
        \end{enumerate}
        
        \item Such that $X \sim \text{Pois}(\lambda)$ Assume we know the parameter value $\lambda$. 
        
        \begin{enumerate}
            \item {\color{red} 
                \begin{align*}
                    P(\mathcal{D} | \lambda) = P(12, 2 | \lambda) &=  \frac{e^{-\lambda} \lambda^{12}}{12!} \cdot  \frac{e^{-\lambda} \lambda^{2}}{2!} \\ 
                    &= \frac{ e^{-2\lambda} \lambda^{12 + 2} }{12! 2!}
                \end{align*}
            }
        \end{enumerate}
        
        \item Such that $X \sim \text{Binom}(20, \theta)$.
        Assume we know the parameter value $\theta$.
        
        \begin{enumerate}
            \item {\color{red} 
                \begin{align*}
                    P(\mathcal{D} | \lambda) = P(12, 2 | \theta) &= \binom{20}{12}\theta^{12}(1-\theta)^{8} \cdot  \binom{20}{2}\theta^{2}(1-\theta)^{18}\\
                    &= \binom{20}{12} \binom{20}{2} \theta^{12+2} (1-\theta)^{ 8+18}
                \end{align*}
            }
        \end{enumerate}
        
    \end{enumerate}
    
    \item Suppose that we collected a dataset with a $n$ observations $\mathcal{D} = \{ x_{1}, x_{2}, \cdots, x_{n} \}$. Please compute the log likelihood if we assume our data was generated by a random variable $X$
        \begin{enumerate}
        \item Such that $X \sim \text{Geom}(p)$. Assume we know the parameter value $p$.
        
        \begin{enumerate}
            \item {\color{red} 
                \begin{align*}
                    P(\mathcal{D} | p) &= P(x_{1},x_{2},\cdots,x_{n} | p)\\
                    &= P(x_{1}|p) \cdot P(x_{2}|p) \cdots P(x_{n}|p)\\
                    &= p(1-p)^{x_{1}-1} \cdot p(1-p)^{x_{2}-1} \cdots p(1-p)^{x_{n}-1} \\ 
                    &= p^{x_{1}+x_{2}+\cdots+x_{n}} (1-p)^{ (x_{1}-1) + (x_{2}-1) + \cdots + (x_{n}-1) } \\ 
                    &= p^{ \sum_{i=1}^{n} x_{i} } (1-p)^{ \sum_{i=1}^{n} (x_{i}-1) } \\ 
                    &= p^{ \sum_{i=1}^{n} x_{i} } (1-p)^{ \sum_{i=1}^{n} x_{i} - n } \\
                    \ell \ell (p) &= \sum_{i=1}^{n} x_{i} \log(p) + \left(\sum_{i=1}^{n} x_{i} - n\right) \log(1-p) 
                \end{align*}
            }
        \end{enumerate}
    
        \item Such that $X \sim \text{Pois}(\lambda)$ Assume we know the parameter value $\lambda$.
        
        \begin{enumerate}
            \item {\color{red} 
                \begin{align*}
                    P(\mathcal{D} | \lambda) &= P(x_{1},x_{2},\cdots,x_{n} | \lambda)\\
                    \mathcal{L}(\lambda) &= P(x_{1} | \lanbda) \cdot P(x_{n} | \lanbda) \cdots P(x_{n} | \lanbda) \\ 
                    \ell \ell (\lambda) &= \sum_{i=1}^{n} \log\left[ P(x_{i} | \lambda) \right] \\ 
                    &= \sum_{i=1}^{n} \log \left[ \frac{e^{-\lambda} \lambda^{x_{i}}}{x_{i}!}  \right] \\ 
                    &= \sum_{i=1}^{n} \log \left[ e^{-\lambda} \lambda^{x_{i}} \right] - \log(x_{i}!)\\
                    &= \sum_{i=1}^{n} \log \left(-\lambda\right) x_{i}\log(\lambda)  - \log(x_{i}!)\\
                    &= \sum_{i=1}^{n} -\log \left(\lambda\right) x_{i}\log(\lambda)  - \log(x_{i}!)\\
                \end{align*}
            }
        \end{enumerate}
        
        
        \item Such that $X \sim \text{Binom}(N, \theta)$.Assume we know the parameter value $\theta$.
        \begin{enumerate}
            \item {\color{red} 
                \begin{align*}
                    P(\mathcal{D} | \theta) &= P(x_{1},x_{2},\cdots,x_{n} | \theta)\\
                    \ell \ell (\theta) &= \sum_{i=1}^{n} \log\left[ P(x_{i} | \theta) \right] \\
                    \ell \ell (\theta) &= \log \left[ \binom{N}{x_{i}} \theta^{x_{i}} (1-\theta)^{20-x_{i}} \right] \\ 
                    \ell \ell (\theta) &= \log \left[ \binom{N}{x_{i}} \right] + x_{i} \log\left[\theta\right] + (20-x_{i})\log\left[1-\theta\right] \right] \\ 
                \end{align*}
            }
        \end{enumerate}
        
        \item Such that $X \sim \mathcal{N}(\mu,\sigma^{2})$.
        Assume we know the parameter values
        
        \begin{enumerate}
            \item {\color{red} 
                \begin{align*}
                    P(\mathcal{D} | p) &= P(x_{1},x_{2},\cdots,x_{n} | p)\\
                    \ell \ell (\theta) &= \sum_{i=1}^{n} \log\left[ f(x_{i} | \theta) \right] \\
                    &= \sum_{i=1}^{n} \log\left\{ \frac{1}{\sqrt{2 \pi \sigma^{2}}} \exp\left[ -\frac{(x_{i} - \mu)^{2}}{2\sigma^{2}} \right]  \right\} \\ 
                    &= \sum_{i=1}^{n} \log\left( \frac{1}{\sqrt{2 \pi \sigma^{2}}} \right) - \frac{(x_{i} - \mu)^{2}}{2 \sigma^{2}} \\ 
                    &= \sum_{i=1}^{n} -\log\left( \sqrt{2 \pi \sigma^{2}} \right) - \frac{(x_{i} - \mu)^{2}}{2 \sigma^{2}} \\ 
                    &= \sum_{i=1}^{n} -\frac{1}{2}\log\left( 2 \pi \sigma^{2} \right) - \frac{(x_{i} - \mu)^{2}}{2 \sigma^{2}} \\ 
                    &=  -\frac{n}{2}\log\left( 2 \pi \sigma^{2} \right) - \sum_{i=1}^{n} \frac{(x_{i} - \mu)^{2}}{2 \sigma^{2}} \\ 
                \end{align*}
            }
        \end{enumerate}
        
    \end{enumerate}
    
    \item Suppose you collect a dataset $\mathcal{D} = (x_{1},x_{2},\cdots,x_{n})$ that you assume is generated from an i.i.d. sample $X_{1}, X_{2},\cdots, X_{n}$, where $X_{i} \sim f(x | \theta)$ and $\theta$ is a parameter.
    You decide to compute the loglikelihood
    \begin{align}
        \ell \ell (\theta | \mathcal{D}) = \sum_{i=1}^{n} \log \left [ f(x_{i} | \theta) \right]
    \end{align}
    Because we are interested in finding the value $\theta^{*}$ that maximizes $\mathcal{\ell \ell}$ then multiplying the log likelihood by a constant will not change the optimal $\theta^{*}$
    \begin{align}
        \frac{1}{n} \ell \ell (\theta | x_{1}, x_{2}, \cdots, x_{n}) = \frac{1}{n} \sum_{i=1}^{n} \log \left [ f(x_{i} | \theta) \right]
    \end{align}
    \begin{enumerate}
        \item The law of large numbers states that the above quantity will approach?
        
        \begin{enumerate}
            \item {\color{red}
                Because 
                \begin{align*}
                    \overline{X_{n}} \to \mathbb{E}(X)
                \end{align*}
                and because
                \begin{align*}
                    \overline{g(X)_{n}} \to \mathbb{E}(g(X))
                \end{align*}
                then we can recognize that $g(x_{i}) = \log\left[f(x_{i}| \theta) \right]$
                So then the above quantity will approach  
                \begin{align}
                    \mathbb{E}\left( \log\left[f(X| \theta) \right]\right)
                \end{align}
            }
        \end{enumerate}
        
    \end{enumerate}
    
    \item Suppose that we are asked to help better understand the burden of influenza circulating in in the state of Pennsylvania. 
    Over the course of 6 weeks we collect the following number of cases of reported influenza $\mathcal{D} = (143, 12, 124, 56, 66)$. 
    \begin{enumerate}
        \item Please propose a model for this data and a short description for why you chose this model. 
        
        \begin{enumerate}
            \item {\color{red} 
                Students can propose many models. Models that are \textbf{not} reasonable (i think) are Bernoulli, Normal. 
                \begin{align*}
                    (X_{1},X_{2},\cdots X_{n})\\
                    X_{i} \sim \text{Pois}(\lambda)
                \end{align*}
            }
        \end{enumerate}
        
        \item Please compute the likelihood for your model parameters given the data
        
        \begin{enumerate}
            \item {\color{red} 
                \begin{align*}
                    P(143, 12, 124, 56, 66 | \lambda) &= \frac{e^{-\lambda} \lambda^{143}}{143!} \cdot \frac{e^{-\lambda} \lambda^{12}}{12!} \cdots \frac{e^{-\lambda} \lambda^{66}}{66!} 
                \end{align*}
        }
        \end{enumerate}
        \item Please compute the log likelihood.
        \begin{enumerate}
            \item {\color{red} 
                \begin{align*}
                    \log\left[P(143, 12, 124, 56, 66 | \lambda)\right] &= -\lambda + 143 \log(\lambda) - \log(143!)\\
                    &-\lambda + 12 \log(\lambda) - \log(12!) + \cdots +\\ &-\lambda + 66 \log(\lambda) - \log(66!)\\
                    &=-5\lambda + \log(\lambda)(143+12+\cdots+66)\\ 
                    &- (\log(143!) + \log(12!) + \cdots \log(66!) )\\
                    &= -5\lambda + \log(\lambda) 401 - (\log(143!) + \log(12!) + \cdots \log(66!) )
                \end{align*}
        }
        \end{enumerate}
        
    \item Consider the sequence $a_{n} = 2^{-n}$ for $n=1$ to $\infty$. 
    \begin{enumerate}
        \item Write down the first 3 items in this sequence 
        
        \begin{enumerate}
            \item  {\color{red} 1/2, 1/4, 1/8  }
        \end{enumerate}
        
        \item Please compute \[ \lim_{n \to \infty} a_{n} \]
        
        \begin{enumerate}
            \item  {\color{red} the limit is the value 0  }
        \end{enumerate}
        
        
    \end{enumerate}
    
    \item Does the infinite sequence $\{-1,1,-1,1,-1,1, \cdots \}$ have a limit? Why or why not? 
    
    \begin{enumerate}
            \item   {\color{red} No. This sequence does not have a limit because this sequence does not approach a single value. Instead, is oscillates. }
        \end{enumerate}
        
    
    \item Consider the finite sequence $a_{n} = 2^{-n}$ for $n=1$ to $100$, a sequence that ends at $2^{-100}$. 
    \begin{enumerate}
        \item Does this sequence have a limit point? Why or why not?
        \begin{enumerate}
            \item  {\color{red} No. This sequence does not have a limit because it is finite. The value 0 is a good candidate for a limit but the sequence cannot get any closer to 0 than 2^{-100} }
        \end{enumerate}
        
        
    \end{enumerate}
    
    \item Compute the derivative of 
    \begin{enumerate}
        \item $f(x) = e^{2x}$
        \begin{enumerate}
            \item  {\color{red} 
            \begin{align}
                f(x) = e^{h(x)} \\ 
                h(x) = 2x\\
                f' = e^{h(x)} (2x)' \\
                f' = e^{h(x)} 2 \\
                f' = 2e^{2x} 
            \end{align}
            }
        \end{enumerate}
        
        \item $f(x) = \log(\frac{1}{2x})$
        \begin{enumerate}
            \item  {\color{red}
            \begin{align}
                f(x) = \log(1/2x) \\ 
                f(x) = \log(h(x)) \\ 
                h(x) = 2x^{-1} \\
                f' = \frac{1}{h(x)} (2x^{-1})' \\
                f' = \frac{1}{h(x)} -1 (2x^{-2}) (2) \\
                f' = -\frac{1}{2x^{-1}} 2x^{-2} (2) \\
                f' = -\frac{2}{x} 
            \end{align}
            }
        \end{enumerate}
        
        \item $f(x) = x^{2} + e^{-x}$
        \begin{enumerate}
            \item  {\color{red} 
            \begin{align}
                f'(x) = 2x - e^{-x}
            \end{align}
            
            }
        \end{enumerate}
        
        \item $f(x) = 10$
        \begin{enumerate}
            \item  {\color{red} f'(x) = 0   }
        \end{enumerate}
        
        \item $f(x) = -3x$
        \begin{enumerate}
            \item  {\color{red}    f'(x) = - 3}
        \end{enumerate}
        
        \item $f(x) = \frac{1}{2}x^{2}$
        \begin{enumerate}
            \item  {\color{red}
                f'(x) = x
            }
        \end{enumerate}
        
        \item $f(x) = \log(x)$
        \begin{enumerate}
            \item  {\color{red}  $ f'(x) = \frac{1}{x}$ }
        \end{enumerate}
        \item $f(x) = \log(x^{2})$
        \begin{enumerate}
            \item  {\color{red} 
            f'(x) = \log(x^{2}) 2x
            
            
            }
        \end{enumerate}
        
        \item $f(x) = \log(10)$
        \begin{enumerate}
            \item  {\color{red}  f'(x) = 0    }
        \end{enumerate}
        
    \end{enumerate}
    
    \item  Assume a set of random variables $Z_{1}, Z_{2}, \cdots, Z_{n}$ such that $Z_{i} \sim \text{exp}(\beta)$ generated an observed dataset. An exponential distribution assign a density over continuous numbers from 0 to infinity. The probability density function is 
    \begin{align*}
        f(z) = \beta e^{-\beta z}     
    \end{align*}
    Suppose we collect a dataset $\mathcal{D} = (2,4,0.5,3)$.
    \begin{enumerate}
        \item Please compute the log likelihood of an arbitrary dataset $\mathcal{D} = (z_{1},z_{2},z_{3}\cdots,z_{n})$
        \begin{enumerate}
            \item {\color{red}
            \begin{align*}
                \ell \ell(\beta) = \log \left \[ p(z_{1},z_{2},\cdots,z_{n}) \r] \\ 
                \ell \ell(\beta) = \log(p(z_{1})+ + \log(p(z_{2})) + \cdots + \log( p(z_{n})) \\ 
                 \ell \ell(\beta) = \log(\beta e^{-\beta z_{1}}) + log(\beta e^{-\beta z_{2}}) + \cdots +  \log(\beta e^{-\beta z_{n}}) \\ 
                 \ell \ell(\beta) = \log(\beta) -\beta z_{1} +  \log(\beta) -\beta z_{2} + \cdots + \log(\beta) -\beta z_{n} \\
                 \ell \ell(\beta) = n\log(\beta) - \beta \sum_{i=1}^{n} z_{i}\\
            \end{align*}
            }
        \end{enumerate}
        
        \item Compute the derivative of the log likelihood above
        \begin{enumerate}
            \item  {\color{red}
            \ell \ell(\beta) = n\log(\beta) - \beta \sum_{i=1}^{n} z_{i}\\
            \ell \ell(\beta)' = \frac{n}{\beta} - \sum_{i=1}^{n} z_{i}
            }
        \end{enumerate}
        
        \item Set the derivative of the log likelihood equal to zero and solve for 
        $\beta$ (ie find the parameter value that maximizes the log likelhiood). 
        \begin{enumerate}
            \item  {\color{red} 
            \begin{align}
            \frac{n}{\beta} - \sum_{i=1}^{n} z_{i} = 0\\
            \frac{n}{\beta}  = \sum_{i=1}^{n} z_{i}\\
            \frac{\beta}{n}  = \frac{1}{\sum_{i=1}^{n} z_{i}}\\
            \beta = \frac{n}{\sum_{i=1}^{n} z_{i}}
            \end{align}
            }
        \end{enumerate}
        \end{enumerate}
        
        \item Apply the maximum likelihood estimate you found to the dataset  $\mathcal{D} = (2,4,0.5,3)$.
        \begin{enumerate}
            \item  {\color{red}$\hat{\beta} = 4/9.5 = 0.42$}
        \end{enumerate}
        
    \end{enumerate}
 
    \item  Assume a set of random variables $Y_{1}, Y_{2}, \cdots, Y_{n}$ such that $Y_{i} \sim \text{Geom}(p)$ generated an observed dataset $\mathcal{D}$. Please compute the maximum likelihood estimate for $p$. Include the log likelihood, derivative, and solution when setting the derivative to zero: 
    \begin{enumerate}
         \item Given a dataset $\mathcal{D} = \{ 12, 1, 0, 3, 8  \}$
        
        \begin{enumerate}
            \item { \color{red}
            \begin{align}
             \hat{p}_{\text{mle}}&=\frac{n}{\sum_{i=1}^{n} y_{i}}  \\ 
             &=5/24 = 0.208
            \end{align}
            }
        \end{enumerate}
        \item Given a dataset $\mathcal{D} = \{y_{1}, y_{2}, \dots, y_{n}\}$
        
        \begin{enumerate}
            \item { \color{red} 
            
            \begin{align}
                f(y) &= p(1-p)^{y-1}\\
                \ell \ell(p) &= \sum_{i=1}^{n} \log \left(p(1-p)^{y_{i}-1} \right) \\ 
                &= \sum_{i=1}^{n} \log (p) + (y_{i}-1) \log(1-p) \\
                \ell \ell '(p) &= \sum_{i=1}^{n} \frac{1}{p} - (y_{i}-1) \frac{1}{1-p}  \\
                \ell \ell '(p) &=  \frac{n}{p} - \sum_{i=1}^{n} y_{i} \frac{1}{1-p} + \frac{n}{1-p}  \\
                \ell \ell '(p) &= 0 \\
                \frac{n}{p} - \sum_{i=1}^{n} y_{i} \frac{1}{1-p} + \frac{n}{1-p} & = 0\\
               \frac{n}{p}  + \frac{n}{1-p} & = \sum_{i=1}^{n} y_{i} \frac{1}{1-p}\\
               (1-p)n + np = p \sum_{i=1}^{n} y_{i}\\
               n = p \sum_{i=1}^{n} y_{i}\\
               \hat{p}_{\text{mle}}=\frac{n}{\sum_{i=1}^{n} y_{i}}
            \end{align}
            
            
            }
        \end{enumerate}
    \end{enumerate}
    
    \item Assume a set of random variables $A_{1}, A_{2}, \cdots, A_{n}$ such that $A_{i} \sim \text{Bern}(\theta)$ generated an observed dataset. Please compute the maximum likelihood estimate for $p$. Include the log likelihood, derivative, and solution when setting the derivative to zero: 
    \begin{enumerate}
        \item Given a dataset $\mathcal{D} = \{ 0,0,0,0 \}$
        
        \begin{enumerate}
            \item { \color{red} Throwing this question out.
            \begin{align}
                \hat{\theta}_{\text{mle}} = \frac{\sum_{i=1}^{n} a_{i}}{n}\\
                \hat{\theta}_{\text{mle}} =0\
            \end{align}
            }
        \end{enumerate}
        \item Do you think the estimate you computed above is reasonable? Why or why not?
        
        \begin{enumerate}
            \item { \color{red} Students can argue for or against whether this is reasonable. i would be exceedingly generous with credit. }
        \end{enumerate}
        
        \item Given a dataset $\mathcal{D} = \{a_{1}, a_{2}, \dots, a_{n}\}$
        
        \begin{enumerate}
            \item { \color{red}
            
            \begin{align}
                f(a) &= \theta^{a}(1-\theta)^{1-a}\\
                \ell \ell (\theta) &= \sum_{i=1}^{n} \log \left[ \theta^{a_{i}}(1-\theta)^{1-a_{i}} \right] \\
                &= \sum_{i=1}^{n} a_{i} \log(\theta) + (1-a_{i} ) \log(1-\theta) \\
                &= \sum_{i=1}^{n} a_{i} \frac{1}{\theta} - (1-a_{i})  \frac{1}{1-\theta}\\
                \ell \ell '(\theta) &= 0 \\
                \sum_{i=1}^{n} a_{i} \frac{1}{\theta} - (1-a_{i})  \frac{1}{1-\theta} &=0\\
                \sum_{i=1}^{n} a_{i} \frac{1}{\theta} = \sum_{i=1}^{n}(1-a_{i})  \frac{1}{1-\theta} \\
                \sum_{i=1}^{n} a_{i} (1-\theta) = \sum_{i=1}^{n}(1-a_{i})  \theta \\
                \sum_{i=1}^{n} a_{i} -\theta \sum_{i=1}^{n} a_{i}  = n\theta  -\theta \sum_{i=1}^{n}a_{i}  \\
                \sum_{i=1}^{n} a_{i}  = n\theta \\
                \hat{\theta}_{\text{mle}} = \frac{\sum_{i=1}^{n} a_{i}}{n}
            \end{align}
            
            }
        \end{enumerate}
    \end{enumerate}
    
    \item Assume a set of random variables $A_{1}, A_{2}, \cdots, A_{n}$ such that $A_{i} \sim \text{Pois}(\lambda)$ generated an observed dataset. Please compute the maximum likelihood estimate for $p$. Include the log likelihood, derivative, and solution when setting the derivative to zero: 
    \begin{enumerate}
        \item Given a dataset $\mathcal{D} = \{ 11,12,2,10 \}$
        
        \begin{enumerate}
            \item { \color{red}  $\hat{\lambda}_{\text{mle}} = 8.75$ }
        \end{enumerate}
        \item Given a dataset $\mathcal{D} = \{a_{1}, a_{2}, \dots, a_{n}\}$
        
        \begin{enumerate}
            \item { \color{red} 
            \begin{align}
                f(a) &= \frac{e^{-\lambda} \lambda ^{a} }{a!}\\
                \ell ell (\lambda) &= \sum_{i=1}^{n} \log \left[\frac{e^{-\lambda} \lambda ^{a_{i}} }{a_{i}!} \right] \\
                &= \sum_{i=1}^{n} -\lambda + a_{i} \log(\lambda) - \log(a_{i}!)\\
                \ell \ell ' (\lambda) &= \sum_{i=1}^{n} -1 + a_{i} \frac{1}{\lambda} \\
                \ell \ell ' (\lambda) &=  -n + \frac{1}{\lambda}\sum_{i=1}^{n}a_{i} \\
                \ell \ell ' (\lambda) &= 0 \\ 
                -n + \frac{1}{\lambda}\sum_{i=1}^{n}a_{i} &= 0 \\
                \frac{1}{\lambda}\sum_{i=1}^{n}a_{i} &= n \\
                \hat{\lambda}_{\text{mle}} = \frac{\sum_{i=1}^{n}a_{i}}{n}
            \end{align}
            }
        \end{enumerate}
    \end{enumerate}

    \item Assume that we model a sample $(X_{1},X_{2},\cdots,X_{n})$ with a Poisson distribution ($X \sim \text{Pois}(\lambda)$). Please derive the Fisher information $\mathcal{I}(\lambda)$ 
        
        \begin{enumerate}
            \item { \color{red} XXX 
            \begin{align}
                f(x) &= \frac{e^{-\lambda} \lambda ^{x} }{x!} \\ 
                \log \left( f(x) \right) &= -\lambda + x log(\lambda) - \log(x!) \\ 
                \log \left( f(x) \right)' &= -1 + \frac{x}{\lambda} \\ 
                \log \left( f(x) \right)'' &= -\frac{x}{\lambda^{2}} \\ 
                \mathbb{E}\left[ \log \left( f(x) \right)''\right] &= -\frac{\mathbb{E}(X)}{\lambda^{2}} \\ &= -\frac{\lambda}{\lambda^{2}} \\ 
                &= -\frac{1}{\lambda} \\ 
                \mathcal{I} = \frac{1}{\lambda}
            \end{align}
            }
        \end{enumerate}
        
        
    \item The number of influenza A and influenza B cases each week is recorded by the Pennsylvania Department of health. 
    For the year 2022 the number of cases in Northampton county, PA is 
    \begin{table}[ht!]
        \centering
        \begin{tabular}{cc}
           Flu A & Flu B  \\
           \hline
            100 & 0 \\ 
            89 & 0 \\ 
            10 & 10 \\ 
            45 & 12 \\ 
            12 & 98 \\ 
        \end{tabular}
    \end{table}
    Please pose a model for the number of influenza A cases, and compute the maximum likelihood estimate for any parameters in this model. 
    Explain why you chose your particular model and explain parameter estimates.
    In addition, use the Fisher information about your parameter to build a 95CI around this estimated parameter value.

        \begin{enumerate}
            \item { \color{red} 
            The model i will pose is model the number of flu cases is $X_{t} \sim \text{Pois}(\lambda)$
            The MLE is $\hat{\lambda}_{\text{mle}} = 51.2$
            $\mathcal{I}_{5} = \frac{5}{51.2}$
            $\hat{\lambda} \sim \mathcal{N}(51.2, 10.2)$
            95CI: $[30.8, 71.6]$
            }
        \end{enumerate}
    \end{enumerate}
    
    
    
    
    
    
    
    
    
    
\end{document}