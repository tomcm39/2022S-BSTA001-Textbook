\chapterauthor{thomas mcandrew}{Lehigh University}
%\chapterauthor{Second Author}{Second Author Affiliation}
\chapter{Likelihood theory}
\hspace{1mm}

\section{Introduction}\label{intro}

There are many different strategies for estimating parameters when given a sample $(X_{1},X_{2}, X_{3}, \cdots, X_{n})$, and a single relation of all the random variables in this sample---what we call a dataset~($\mathcal{D}$).
Each strategy for estimating parameters has advantages and disadvantages. 
Up until this point we learned a single strategy called \textit{The Method of Moments}.
Lets explore a second, popular strategy for estimating parameters given a sample and a model called \textit{The likelihood approach}.

The likelihood approach has as advantages that estimatros are often simpler to compute than the method of moments, we can construct a straightforward method to compute the variance of estimators which is based on the curvature of a likelihood functions, and XXX.

\section{Probability of a dataset}

Suppose that we are provided a single dataset $\mathcal{D} = (y_{1},y_{2},\cdots,y_{n})$, and we assume that this dataset was generated from a sample $(Y_{1},Y_{2},Y_{3},\cdots,Y_{n})$.

The \textbf{Probability of our dataset}---$P(\mathcal{D})$---is defined as 
\begin{align}
    P(\mathcal{D}) &= P( Y_{1}=y_{1} \cap Y_{2}=y_{2} \cap \cdots \cap Y_{n}=y_{n} )\\
                   &=P( Y_{1}=y_{1} , Y_{2}=y_{2} , \cdots , Y_{n}=y_{n} )
\end{align}
where we use a comma to denote the $\cap$ symbol.
The probability of our dataset is the probability that random variable $Y_{1}$ generates the realized value $y_{1}$, $Y_{2}$ generates $y_{2}$ , and so on, all simultaneously.

If we assume that our sample is independent and identically distributed (i.i.d.) then we can simplify $P(\mathcal{D})$. 

We need one fact first. 

\subsubsection{Conditional probability chains}

Consider computing the probability of a sample of size three $(X_{1},X_{2},X_{3})$.
\begin{equation}
    P(X_{1}=x_{1} \cap X_{2}=x_{2} \cap X_{3}=x_{3})
\end{equation}

We can imagine that inside the \textbf{event} $X_{1}=x_{1} \cap X_{2}=x_{2} \cap X_{3}=x_{3}$ there exists two events. 
\begin{equation}
    [X_{1}=x_{1}] \cap [X_{2}=x_{2} \cap X_{3}=x_{3}]
\end{equation}
Here, we partitioned the above event that contained three random variables into two events: one event that contains a single random variable and a second event that contains two random variables.

Now we can use an identity that we learned in the past to break this probability into two.
For two events $A$ and $B$, recall  
\begin{equation}
    P(A \cap B) = P(B|A) P(A).
\end{equation}
Then
\begin{align}
    &P( [X_{1}=x_{1}] \cap [X_{2}=x_{2} \cap X_{3}=x_{3}]) = \\
    &P( X_{1}=x_{1} |  [X_{2}=x_{2} \cap X_{3}=x_{3}]) P(X_{2}=x_{2} \cap X_{3}=x_{3}). 
\end{align}
We could apply the same technique above to the event $X_{2}=x_{2} \cap X_{3}=x_{3}$ and find that 
\begin{align}
    &P( X_{1}=x_{1} |  [X_{2}=x_{2} \cap X_{3}=x_{3}]) P(X_{2}=x_{2} \cap X_{3}=x_{3}) =  \\
    &P( X_{1}=x_{1} |  [X_{2}=x_{2} \cap X_{3}=x_{3}]) P(X_{2}=x_{2} |  X_{3}=x_{3}) P(X_{3}=x_{3})  
\end{align}
The process of breaking a single probability into several probabilities in this way is sometimes called producing a conditional chain of probabilities. 


\subsection{Probability of an i.i.d. sample}

Given an i.i.d. sample $(Y_{1},Y_{2},\cdots,Y_{n})$ the probability of this sample is 
\begin{align}
    P(Y_{1} = y_{1}, Y_{2} = y_{2}, \cdots Y_{n} = y_{n})
\end{align}
We can break this probability into several conditional probabilities.
\begin{align}
    &P(Y_{1} = y_{1}, Y_{2} = y_{2}, \cdots Y_{n} = y_{n}) =\\  &P(Y_{1} | Y_{2} \cap Y_{3} \cap \cdots Y_{n} ) P( Y_{2} | Y_{3} \cap \cdots Y_{n} ) P(Y_{3} | Y_{4} \cap \cdots Y_{n}) \cdots \\
    &P(Y_{n-1} | Y_{n}) P(Y_{n})
\end{align}
But because we assume our sample is (mutually) independent then 
\begin{align}
     &P(Y_{1} | Y_{2} \cap Y_{3} \cap \cdots Y_{n} ) P( Y_{2} | Y_{3} \cap \cdots Y_{n} ) P(Y_{3} | Y_{4} \cap \cdots\\
     &Y_{n}) \cdots P(Y_{n-1} | Y_{n}) P(Y_{n}) = P(Y_1) P(Y_{2}) \cdots P(Y_{n})
\end{align}

To recap, the probability of a dataset generated by an i.i.d. sample is 
\begin{align}
    &P(X_{1} = x_{1}, X_{2} = x_{2}, X_{3} = x_{3}, \cdots X_{n} = x_{n}   ) =\\
    &P(X_{1}=x_{1}) P(X_{2}=x_{2}) P(X_{3}=x_{3})\cdots P(X_{n}=x_{n}) \\ 
    &= \prod_{i=1}^{N} P(X_{i} = x_{i})
\end{align}

\ex Suppose an i.i.d. sample $(Y_{1},Y_{2},\cdots, Y_{n})$ generated a dataset $\mathcal{D} = (y_{1},y_{2},\cdots,y_{n})$. Further assume $Y_{i} \sim \text{Bern}(\theta)$ for $i=1$ to $N$. Then

\begin{align}
    P(\mathcal{D} | \theta ) &= P(Y_{1}=y_{1}) P(Y_{2}=y_{2}) \cdots P(Y_{n}=y_{n}) \\ 
                             &= \left[\theta^{y_{1}}(1 - \theta)^{1-y_{1}}\right]\left[\theta^{y_{2}}(1 - \theta)^{1-y_{2}}\right] \cdots \left[\theta^{y_{n}}(1 - \theta)^{1-y_{n}}\right] \\ 
                             &= \prod_{i=1}^{N} \left[\theta^{y_{i}}(1 - \theta)^{1-y_{i}}\right] \\ 
                             &= \theta^{ \sum_{i=1}^{N} y_{i}  } (1-\theta)^{N - \sum_{i=1}^{N} y_{i}  }
\end{align}

\section{ The likelihood function }

The probability of our dataset $P(\mathcal{D} )$ depends on a set of parameters $(\theta)$, and we can write $P(\mathcal{D} | \theta)$.
Different choices for our parameter values will result in different values of $P(\mathcal{D} | \theta)$. 

\ex (Bernoulli example continued). Suppose that we are given the dataset $\mathcal{D} = (1,0,1,0,0,1,1)$ and we assume that these 7 datapoints were generated from an i.i.d. Bernoulli model. Then, as we saw above, 
\begin{equation}
    P(\mathcal{D} | \theta) = \theta^{ \sum_{i=1}^{N} y_{i}  } (1-\theta)^{N - \sum_{i=1}^{N} y_{i} }    
\end{equation}. 
For our dataset, we can fill in the $y_{i}$ values to find  
\begin{equation}
    P(\mathcal{D} | \theta) = \theta^{ 4 } (1-\theta)^{7 - 4 }    
\end{equation}
We can now compute the probability of our data for different choices of $\theta$
\begin{align}
    P(\mathcal{D} | \theta=0.25) = 0.25^{ 4 } (1-0.25)^{3 } = \\
    P(\mathcal{D} | \theta=0.50) = 0.50^{ 4 } (1-0.50)^{3 } = \\
    P(\mathcal{D} | \theta=0.75) = 0.75^{ 4 } (1-0.75)^{3 } = 
\end{align}

The \textbf{likelihood function}, $\mathcal{L}$, is the probability of our observed data as a function of a set of parameter values
\begin{equation}
    \mathcal{L}(\theta | \mathcal{D}) = P(\mathcal{D} | \theta)
\end{equation}
The difference between $P(\mathcal{D} | \theta)$ and $\mathcal{L}(\theta | \mathcal{D})$ is the argument.
For $P(\mathcal{D} | \theta)$ we expect, given a set of fixed parameter values, to compute the probability of different datasets. 
For $\mathcal{L}(\theta | \mathcal{D})$ we expect, given a fixed, observed dataset~$\mathcal{D}$, to compute the probability of $\mathcal{D}$ for different values of $\theta$.

The likelihood function allows us to investigate the plausability of different parameter values after we observed a dataset. 
The likelihood need not be a probability distribution, and often is not. 

\ex Suppose we observe for 7 days the number of patients who arrive at a specific hospital and are diagnosed with influenza. The data we collect is $\mathcal{D} = (10,13,4,17,34,11,3)$. We choose to consider our data a random sample $(X_{1},X_{2},\cdots,X_{7})$ where $X_{i} \sim \text{Pois}(\lambda)$.
Then
\begin{align}
    P(\mathcal{D} | \lambda) &= \prod_{i=1}^{7} f( x_{i} | \lambda) \\ 
                             &= \prod_{i=1}^{7} \frac{e^{-\lambda} \lambda^{x_{i}}}{x_{i} !} \\
                             &= \frac{e^{-7\lambda} \lambda^{10+13+\cdots+3}}{10! 13! 4! \cdots 3!}
\end{align} 

If we consider the above as a function of $\lambda$---$\mathcal{L}(\lambda | \mathcal{D})$ then if $\mathcal{L}(\lambda_{1} | \mathcal{D})$ > $\mathcal{L}(\lambda_{2} | \mathcal{D})$ then the parameter value $\lambda_{1}$ may be a more plausible parameter value compared to $\lambda_{2}$. 

One way to use the likelihood function is to attempt to find the most plausible parameter value, the value that maximizes the likelihood. 
However this value may be difficult to find. 
Lets explore one strategy for simplifying the search for this optimal parameter value and a second strategy that can help us compute this optimal value.

\section{ The logarithm and the log likelihood }
With a likelihood function in hand, one aim may be to find the set of parameter values that maximizes this function---the set of parameter values that are more plausible given an observed dataset. 
However, the likelihood function $\mathcal{L}$ can in many cases be difficult to compute because it is a sequence of multiplications.
Our aims is to find a related function $\ell \ell$ such that parameter values that maximize $\mathcal{L}$ also maximize this "easier to work with" $\ell \ell$ function. 

A function $g$ is \textbf{monotone increasing} if for two values $x>y$ then $g(x) \geq g(y)$. 
If a monotone function $g$ is applied to a second function $f$, the values that maximize $f$ also maximize this new function. 

\textit{Hypothesis:} 
Suppose the value $x^{*}$ maximizes the function $f$.
That is, for all $x$ the value $f(x^{*}) \geq f(x)$. 
Then if $g$ is a monotone increasing function then $x^{*}$ also maximizes $g(f(x))$.

\textit{Evidence:}
We know that $f(x^{*}) \geq f(x)$.
$g$ is a monotone increasing function and so by definition for $x \geq y$, $g(x)>g(y)$. 
We can use as an input to $g$ the values $f(x^{*})$ and $f(x)$ to find that because $f(x^{*}) \geq f(x)$ then $g( f(x^{*}) ) \geq g( f(x) )$. 

The above property of a monotone increasing function is useful if we can find such a function that simplifies our likelihood---a sequence of multiplications. 
The logarithm is a monotone increasing function (i.e.  if $x \geq y$ then $\log(x) \geq \log(y)$ ), and the logarithm transforms multiplications~(hard) into sums~(easy).

The \textbf{log likelihood} function is the natural logarithm of the likelihood function
\begin{equation}
    \ell \ell (\theta | \mathcal{D}) = \log \left[ \mathcal{L} (\theta | \mathcal{D})  \right]
\end{equation}

The loglikelihood simplifies the likelihood by transforming a large sequence of multiplications into sums.

There is one more principle about maximizing functions that will be useful. 
Suppose we find the value $(x^{*})$ that maximizes the function 
\begin{equation}
    g(x) = f(x) + c
\end{equation}
then that value $x^{*}$ also maximizes $f(x)$.
Also, suppose we find the value $(x^{*})$ that maximizes the function 
\begin{equation}
    g(x) = c \cdot f(x)
\end{equation}
then that value $x^{*}$ also maximizes $f(x)$.

The above two principles state that if we are interested in finding the maximum value of a function $g$ then we can consider a simpler function $f$ which does not contain unrelated constants.

\ex The value $x^{*}$ that maximizes the function $g(x) = -x^{2} + 3$ is the same value that maximizes the function $f(x) = -x^{2}$. 

\ex The value $x^{*}$ that maximizes the function $g(x) = \frac{e^{-x^{2}}}{4}$ is the same value that maximizes the function $f(x) = e^{-x^{2}}$. 


\ex We found in the previous section that the likelihood for the number of admitted patients with influenza was
\begin{equation}
\mathcal{L}(\lambda | \mathcal{D}) = \frac{e^{-7\lambda} \lambda^{10+13+\cdots+3}}{10! 13! 4! \cdots 3!}.
\end{equation}
The log likelihood of the above is, the simpler, 
\begin{align}
    \ell \ell (\lambda | \mathcal{D}) &= \log \left[ \frac{e^{-7\lambda} \lambda^{10+13+\cdots+3}}{10! 13! 4! \cdots 3!} \right] \\ 
    &= -7 \lambda + (10+13+\cdots+3) \log(\lambda) - \left[ \log(10!) + \log(13!) + \cdots + \log(3!) \right]
\end{align}
We see that the last term of factorials does not involve our parameter value $\lambda$. 
This term is then a constant and we could instead aim to find the maximium of the simpler function 
\begin{align}
    \ell \ell (\lambda | \mathcal{D}) &= -7 \lambda + (10+13+\cdots+3) \log(\lambda) \\ 
    &=  -7 \lambda + 83\log(\lambda)
\end{align}

In some cases, we may be able to find the parameter value, or set of parameter values, that maximize the log likelihood function. 
To explore this topic we will need to briefly learn about the derivative of a continuous function. 

\section{ Just in time derivatives }

The derivative describes the tangential slope of a function at a point, and is an excellent linear approximation to a complicated function.
We will see how we can use the derivative to setup and solve an equation that will return a parameter value that maximizes the log likelihood. 
This estimate of our parameter will be called a \textbf{maximum likelihood estimate}.

\subsection{Sequences and limits}

A sequence is defined as a function whose domain is the natural numbers $\mathbb{N} = \{1,2,3,4,5,6, \cdots  \}$. 
Often as annotate the value associated with the 1, $f(1)$ as $a_{1}$, the value associated with 2, $f(2)$, as $a_{2}$, and so on.

\ex The following is a sequence $f(x) = 2^{-x}$ for $x \in \mathbb{N}$.The first few values of this sequence are $f(1) = a_{1} = 1/2$, $f(2) = a_{2} = 1/4$, $f(3) = a_{3} = 1/8$, and so on. We can also describe the above sequence using this notation $\{ 2^{-n} \}_{n=1}^{\infty}$.  

Intuitively, we may wish to know how a sequence behaves as it is enumerated. 
A sequence may increase to infinity, decrease to negative infinity, oscillate or have some other periodic behavior, or a sequence may move closer and closer to a single value. 
If a sequence ${a_{n}}_{n=1}^{\infty}$ gets closer and closer to a single value $l$ then we say that the sequence has a \textbf{limit} and that limit is the value $l$.

The more formal definition of a limit is the following:
For a sequence $\{a_{n}\}_{n=1}^{\infty}$, given any value $\epsilon >0$ the value $l$ is a limit if you can find an integer $N$ such that $|a_{N} - l| < \epsilon$ for integers greater than $N$ (i.e. $N+1$, $N+2$, $\cdots$).

We may also define a limit $L$ of a function at the value $a$ if given a sequence of values $a_{1}, a_{2}, \cdots a_{n}$ that approach the value $a$, the sequence $f(a_{1}), f(a_{2}), f(a_{3}), \cdots$ approaches the value $L$. We call $L$ the limit of $f$ at the value $a$ and write \begin{equation}
    \lim_{x \to a} f(x) = L
\end{equation}
Intuitively, the above says that as the input of our function approaches the value $a$, the function values approach the value $L$.

To solve our problem of finding the parameter value that maximizes the log likelihood, we need to learn about a special type of limit called the derivative.

\subsection{Derivative of a function}

The derivative of a function at the value $a$, or $f'(a)$, is 
\begin{align}
    f'(a) = \lim_{ x \to a } \frac{f(x) - f(a)}{x-a}
\end{align}

For intuition, we can imagine building a sequence
\begin{equation}
    a_{1},a_{2},a_{3},\cdots, a_{n} \to a
\end{equation} 
that approaches the value $a$ and computing the associated sequence
\begin{equation}
\frac{f(a_{1}) - f(a)}{a_{1}-a},\frac{f(a_{2}) - f(a)}{a_{2}-a},\frac{f(a_{3}) - f(a)}{a_{3}-a},\cdots,\frac{f(a_{n}) - f(a)}{a_{n}-a} \to f'(a)
\end{equation}

One common sequence that approaches $a$ is the sequence $a + \{h_{n}\}_{n=1}^{\infty}$ and where the sequence $h$ approaches $0$.
Then the definition of the derivative would be 
\begin{align}
    f'(a) &= \lim_{ h \to 0 } \frac{f(a+h) - f(a)}{a+h-a} \\ 
          &= \lim_{ h \to 0 } \frac{f(a+h) - f(a)}{h} 
\end{align}
This is because as the sequence $\{h_{n}\}$ approaches zero, the sequence $\{a+h_{n}\}$ approaches the value $a$.

\ex Let $g(x) = x^{2}$. The derivative of $g$ at the value 1 is by definition
\begin{align}
    g'(1) &= \lim_{ h \to 0 } \frac{g(1+h) - g(1) }{h} \\ 
          &= \lim_{ h \to 0 } \frac{ (1+h)^{2} - 1^{2} }{h}\\
          &= \lim_{ h \to 0 } \frac{ h^{2} + 2h + 1 - 1 }{h}\\
          &= \lim_{ h \to 0 } \frac{ h^{2} + 2h }{h}\\
          &= \lim_{ h \to 0 } \frac{ h + 2 }{1}\\
          &=2
\end{align}
For any value $c$ we find that the derivative of $g$ is 
\begin{align}
    g'(c) &= \lim_{ h \to 0 } \frac{g(c+h) - g(c) }{h} \\ 
          &= \lim_{ h \to 0 } \frac{ (c+h)^{2} - c^{2} }{h}\\
          &= \lim_{ h \to 0 } \frac{ h^{2} + 2hc + c^{2} - c^{2} }{h}\\
          &= \lim_{ h \to 0 } \frac{ h^{2} + 2hc }{h}\\
          &= \lim_{ h \to 0 } \frac{ h + 2c }{1}\\
          &=2c
\end{align}

In some sense, the derivative $g'(c)$ too is a function, the function $g'(c) = 2c$.

\subsection{When the derivative equals zero}

When the derivative of a function at the point $x^{*}$ is 0 the point $x^{*}$ is either a minimum or a maximum value. 
This is the key to finding a parameter value that maximizes our log likelihood. 
If we can compute the derivative of our log likelihood $\ell \ell'(\theta)$ and find the value $\theta$ such that $\ell \ell' = 0$ then we know we have found either a parameter value that minimizes or maximizes our log likelhood, which then minimizes or maximizes our likelihood, which suggests that this parameter value is the most plausible value given our observed data. Phew. 

Lets prove that when the derivative equal zero then we are at a maximum. 
\textit{Hypothesis}: If $f(x^{*}) > f(x)$ for any possible input $x$ then  $f'(x^{*})=0$.
\textit{Evidence:} By definition
\begin{align}
    f'(x^{*}) &= \lim_{x \to x^{*}} \frac{ f(x) - f(x^{*}) }{x-x^{*}} = 0 \\ 
\end{align}

Let us build a sequence of values $\{x_{1}, x_{2}, \cdots \}$ that approaches $x^{*}$ such that all the values are smaller than $x^{*}$. Then if we choose any $x_{n}$ in the sequence, the following must be true:
\begin{align}
    \frac{f(x_{n}) - f(x^{*})}{x_{n}-x^{*}} > 0 & \;\; \text{bc $f(x^{*})$ is a max} \\ 
    \frac{f(x_{n}) - f(x^{*})}{x_{n}-x^{*}} =c  & \;\;  \text{c is some positive value}\\
    f(x_{n}) - f(x^{*}) =c (x_{n}-x^{*})        & \;\;  \text{algebra}\\
    f(x_{n}) - c (x_{n}-x^{*}) = f(x^{*})       & \;\;  \text{algebra}  \\
\end{align}

Because our sequence $\{x_{n}\}_{n=1}^{\infty}$ is always smaller than $x^{*}$, the term  $(x_{n}-x^{*})$ is negative, and so then
\begin{align}
    f(x_{n}) + v &= f(x^{*}) \\ 
    f(x_{n}) &< f(x^{*})
\end{align}

Instead of a sequence that is always smaller than $x^{*}$ we could have chosen a sequence that is always larger than $x^{*}$.
\begin{align}
    \frac{f(x_{n}) - f(x^{*})}{x_{n}-x^{*}} < 0 \\ 
    \frac{f(x_{n}) - f(x^{*})}{x_{n}-x^{*}} = -c \\
    f(x_{n}) - f(x^{*}) = -c (x_{n}-x^{*}) \\
    f(x_{n}) + c (x_{n}-x^{*}) = f(x^{*})  \\
\end{align}

Because the sequence $\{x_{n}\}_{n=1}^{\infty}$ is always larger than $x^{*}$ the quantity $(x_{n}-x^{*})$ is positive. Then 
\begin{align}
    f(x_{n}) + v &= f(x^{*}) \\ 
    f(x_{n}) &< f(x^{*})
\end{align}


[NEED PICTURE HERE]


\subsection{derivative "rules"}



\section{ Estimating variance, curvature of the likelihood, and Fishers Information }


\section{ Evaluating estimators }





