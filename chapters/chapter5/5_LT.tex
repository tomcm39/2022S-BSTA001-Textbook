\chapterauthor{thomas mcandrew}{Lehigh University}
%\chapterauthor{Second Author}{Second Author Affiliation}
\chapter{Likelihood theory}
\hspace{1mm}

\section{Introduction}\label{intro}

There are many different strategies for estimating parameters when given a sample $(X_{1},X_{2}, X_{3}, \cdots, X_{n})$, and a single relation of all the random variables in this sample---what we call a dataset~($\mathcal{D}$).
Each strategy for estimating parameters has advantages and disadvantages. 
Up until this point we learned a single strategy called \textit{The Method of Moments}.
Lets explore a second, popular strategy for estimating parameters given a sample and a model called \textit{The likelihood approach}.

The likelihood approach has as advantages that estimators are often simpler to compute than the method of moments, we can construct a straightforward method to compute the variance of estimators which is based on the curvature of a likelihood functions, and choosing parameters that "best fit" the data is an intuitive approach.

\section{Probability of a dataset}

Suppose that we are provided a single dataset $\mathcal{D} = (y_{1},y_{2},\cdots,y_{n})$, and we assume that this dataset was generated from a sample $(Y_{1},Y_{2},Y_{3},\cdots,Y_{n})$.

The \textbf{Probability of our dataset}---$P(\mathcal{D})$---is defined as 
\begin{align}
    P(\mathcal{D}) &= P( Y_{1}=y_{1} \cap Y_{2}=y_{2} \cap \cdots \cap Y_{n}=y_{n} )\\
                   &=P( Y_{1}=y_{1} , Y_{2}=y_{2} , \cdots , Y_{n}=y_{n} )
\end{align}
where we use a comma to denote the $\cap$ symbol.
The probability of our dataset is the probability that random variable $Y_{1}$ generates the realized value $y_{1}$, $Y_{2}$ generates $y_{2}$ , and so on, all simultaneously.

If we assume that our sample is independent and identically distributed (i.i.d.) then we can simplify $P(\mathcal{D})$. 

We need one fact first. 

\subsubsection{Conditional probability chains}

Consider computing the probability of a sample of size three $(X_{1},X_{2},X_{3})$.
\begin{equation}
    P(X_{1}=x_{1} \cap X_{2}=x_{2} \cap X_{3}=x_{3})
\end{equation}

We can imagine that inside the \textbf{event} $X_{1}=x_{1} \cap X_{2}=x_{2} \cap X_{3}=x_{3}$ there exists two events. 
\begin{equation}
    [X_{1}=x_{1}] \cap [X_{2}=x_{2} \cap X_{3}=x_{3}]
\end{equation}
Here, we partitioned the above event that contained three random variables into two events: one event that contains a single random variable and a second event that contains two random variables.

Now we can use an identity that we learned in the past to break this probability into two.
For two events $A$ and $B$, recall  
\begin{equation}
    P(A \cap B) = P(B|A) P(A).
\end{equation}
Then
\begin{align}
    &P( [X_{1}=x_{1}] \cap [X_{2}=x_{2} \cap X_{3}=x_{3}]) = \\
    &P( X_{1}=x_{1} |  [X_{2}=x_{2} \cap X_{3}=x_{3}]) P(X_{2}=x_{2} \cap X_{3}=x_{3}). 
\end{align}
We could apply the same technique above to the event $X_{2}=x_{2} \cap X_{3}=x_{3}$ and find that 
\begin{align}
    &P( X_{1}=x_{1} |  [X_{2}=x_{2} \cap X_{3}=x_{3}]) P(X_{2}=x_{2} \cap X_{3}=x_{3}) =  \\
    &P( X_{1}=x_{1} |  [X_{2}=x_{2} \cap X_{3}=x_{3}]) P(X_{2}=x_{2} |  X_{3}=x_{3}) P(X_{3}=x_{3})  
\end{align}
The process of breaking a single probability into several probabilities in this way is sometimes called producing a conditional chain of probabilities. 


\subsection{Probability of an i.i.d. sample}

Given an i.i.d. sample $(Y_{1},Y_{2},\cdots,Y_{n})$ the probability of this sample is 
\begin{align}
    P(Y_{1} = y_{1}, Y_{2} = y_{2}, \cdots Y_{n} = y_{n})
\end{align}
We can break this probability into several conditional probabilities.
\begin{align}
    &P(Y_{1} = y_{1}, Y_{2} = y_{2}, \cdots Y_{n} = y_{n}) =\\  &P(Y_{1} | Y_{2} \cap Y_{3} \cap \cdots Y_{n} ) P( Y_{2} | Y_{3} \cap \cdots Y_{n} ) P(Y_{3} | Y_{4} \cap \cdots Y_{n}) \cdots \\
    &P(Y_{n-1} | Y_{n}) P(Y_{n})
\end{align}
But because we assume our sample is (mutually) independent then 
\begin{align}
     &P(Y_{1} | Y_{2} \cap Y_{3} \cap \cdots Y_{n} ) P( Y_{2} | Y_{3} \cap \cdots Y_{n} ) P(Y_{3} | Y_{4} \cap \cdots\\
     &Y_{n}) \cdots P(Y_{n-1} | Y_{n}) P(Y_{n}) = P(Y_1) P(Y_{2}) \cdots P(Y_{n})
\end{align}

To recap, the probability of a dataset generated by an i.i.d. sample is 
\begin{align}
    &P(X_{1} = x_{1}, X_{2} = x_{2}, X_{3} = x_{3}, \cdots X_{n} = x_{n}   ) =\\
    &P(X_{1}=x_{1}) P(X_{2}=x_{2}) P(X_{3}=x_{3})\cdots P(X_{n}=x_{n}) \\ 
    &= \prod_{i=1}^{N} P(X_{i} = x_{i})
\end{align}

\ex Suppose an i.i.d. sample $(Y_{1},Y_{2},\cdots, Y_{n})$ generated a dataset $\mathcal{D} = (y_{1},y_{2},\cdots,y_{n})$. Further assume $Y_{i} \sim \text{Bern}(\theta)$ for $i=1$ to $N$. Then

\begin{align}
    P(\mathcal{D} | \theta ) &= P(Y_{1}=y_{1}) P(Y_{2}=y_{2}) \cdots P(Y_{n}=y_{n}) \\ 
                             &= \left[\theta^{y_{1}}(1 - \theta)^{1-y_{1}}\right]\left[\theta^{y_{2}}(1 - \theta)^{1-y_{2}}\right] \cdots \left[\theta^{y_{n}}(1 - \theta)^{1-y_{n}}\right] \\ 
                             &= \prod_{i=1}^{N} \left[\theta^{y_{i}}(1 - \theta)^{1-y_{i}}\right] \\ 
                             &= \theta^{ \sum_{i=1}^{N} y_{i}  } (1-\theta)^{N - \sum_{i=1}^{N} y_{i}  }
\end{align}

\section{ The likelihood function }

The probability of our dataset $P(\mathcal{D} )$ depends on a set of parameters $(\theta)$, and we can write $P(\mathcal{D} | \theta)$.
Different choices for our parameter values will result in different values of $P(\mathcal{D} | \theta)$. 

\ex (Bernoulli example continued). Suppose that we are given the dataset $\mathcal{D} = (1,0,1,0,0,1,1)$ and we assume that these 7 datapoints were generated from an i.i.d. Bernoulli model. Then, as we saw above, 
\begin{equation}
    P(\mathcal{D} | \theta) = \theta^{ \sum_{i=1}^{N} y_{i}  } (1-\theta)^{N - \sum_{i=1}^{N} y_{i} }    
\end{equation}. 
For our dataset, we can fill in the $y_{i}$ values to find  
\begin{equation}
    P(\mathcal{D} | \theta) = \theta^{ 4 } (1-\theta)^{7 - 4 }    
\end{equation}
We can now compute the probability of our data for different choices of $\theta$
\begin{align}
    P(\mathcal{D} | \theta=0.25) = 0.25^{ 4 } (1-0.25)^{3 } = \\
    P(\mathcal{D} | \theta=0.50) = 0.50^{ 4 } (1-0.50)^{3 } = \\
    P(\mathcal{D} | \theta=0.75) = 0.75^{ 4 } (1-0.75)^{3 } = 
\end{align}

The \textbf{likelihood function}, $\mathcal{L}$, is the probability of our observed data as a function of a set of parameter values
\begin{equation}
    \mathcal{L}(\theta | \mathcal{D}) = P(\mathcal{D} | \theta)
\end{equation}
The difference between $P(\mathcal{D} | \theta)$ and $\mathcal{L}(\theta | \mathcal{D})$ is the argument.
For $P(\mathcal{D} | \theta)$ we expect, given a set of fixed parameter values, to compute the probability of different datasets. 
For $\mathcal{L}(\theta | \mathcal{D})$ we expect, given a fixed, observed dataset~$\mathcal{D}$, to compute the probability of $\mathcal{D}$ for different values of $\theta$.

The likelihood function allows us to investigate the plausability of different parameter values after we observed a dataset. 
The likelihood need not be a probability distribution, and often is not. 

\ex Suppose we observe for 7 days the number of patients who arrive at a specific hospital and are diagnosed with influenza. The data we collect is $\mathcal{D} = (10,13,4,17,34,11,3)$. We choose to consider our data a random sample $(X_{1},X_{2},\cdots,X_{7})$ where $X_{i} \sim \text{Pois}(\lambda)$.
Then
\begin{align}
    P(\mathcal{D} | \lambda) &= \prod_{i=1}^{7} f( x_{i} | \lambda) \\ 
                             &= \prod_{i=1}^{7} \frac{e^{-\lambda} \lambda^{x_{i}}}{x_{i} !} \\
                             &= \frac{e^{-7\lambda} \lambda^{10+13+\cdots+3}}{10! 13! 4! \cdots 3!}
\end{align} 

If we consider the above as a function of $\lambda$---$\mathcal{L}(\lambda | \mathcal{D})$ then if $\mathcal{L}(\lambda_{1} | \mathcal{D})$ > $\mathcal{L}(\lambda_{2} | \mathcal{D})$ then the parameter value $\lambda_{1}$ may be a more plausible parameter value compared to $\lambda_{2}$. 

One way to use the likelihood function is to attempt to find the most plausible parameter value, the value that maximizes the likelihood. 
However this value may be difficult to find. 
Lets explore one strategy for simplifying the search for this optimal parameter value and a second strategy that can help us compute this optimal value.

\section{ The logarithm and the log likelihood }
With a likelihood function in hand, one aim may be to find the set of parameter values that maximizes this function---the set of parameter values that are more plausible given an observed dataset. 
However, the likelihood function $\mathcal{L}$ can in many cases be difficult to compute because it is a sequence of multiplications.
Our aims is to find a related function $\ell \ell$ such that parameter values that maximize $\mathcal{L}$ also maximize this "easier to work with" $\ell \ell$ function. 

A function $g$ is \textbf{monotone increasing} if for two values $x>y$ then $g(x) \geq g(y)$. 
If a monotone function $g$ is applied to a second function $f$, the values that maximize $f$ also maximize this new function. 

\textit{Hypothesis:} 
Suppose the value $x^{*}$ maximizes the function $f$.
That is, for all $x$ the value $f(x^{*}) \geq f(x)$. 
Then if $g$ is a monotone increasing function then $x^{*}$ also maximizes $g(f(x))$.

\textit{Evidence:}
We know that $f(x^{*}) \geq f(x)$.
$g$ is a monotone increasing function and so by definition for $x \geq y$, $g(x)>g(y)$. 
We can use as an input to $g$ the values $f(x^{*})$ and $f(x)$ to find that because $f(x^{*}) \geq f(x)$ then $g( f(x^{*}) ) \geq g( f(x) )$. 

The above property of a monotone increasing function is useful if we can find such a function that simplifies our likelihood---a sequence of multiplications. 
The logarithm is a monotone increasing function (i.e.  if $x \geq y$ then $\log(x) \geq \log(y)$ ), and the logarithm transforms multiplications~(hard) into sums~(easy).

The \textbf{log likelihood} function is the natural logarithm of the likelihood function
\begin{equation}
    \ell \ell (\theta | \mathcal{D}) = \log \left[ \mathcal{L} (\theta | \mathcal{D})  \right]
\end{equation}

The loglikelihood simplifies the likelihood by transforming a large sequence of multiplications into sums.

There is one more principle about maximizing functions that will be useful. 
Suppose we find the value $(x^{*})$ that maximizes the function 
\begin{equation}
    g(x) = f(x) + c
\end{equation}
then that value $x^{*}$ also maximizes $f(x)$.
Also, suppose we find the value $(x^{*})$ that maximizes the function 
\begin{equation}
    g(x) = c \cdot f(x)
\end{equation}
then that value $x^{*}$ also maximizes $f(x)$.

The above two principles state that if we are interested in finding the maximum value of a function $g$ then we can consider a simpler function $f$ which does not contain unrelated constants.

\ex The value $x^{*}$ that maximizes the function $g(x) = -x^{2} + 3$ is the same value that maximizes the function $f(x) = -x^{2}$. 

\ex The value $x^{*}$ that maximizes the function $g(x) = \frac{e^{-x^{2}}}{4}$ is the same value that maximizes the function $f(x) = e^{-x^{2}}$. 


\ex We found in the previous section that the likelihood for the number of admitted patients with influenza was
\begin{equation}
\mathcal{L}(\lambda | \mathcal{D}) = \frac{e^{-7\lambda} \lambda^{10+13+\cdots+3}}{10! 13! 4! \cdots 3!}.
\end{equation}
The log likelihood of the above is, the simpler, 
\begin{align}
    \ell \ell (\lambda | \mathcal{D}) &= \log \left[ \frac{e^{-7\lambda} \lambda^{10+13+\cdots+3}}{10! 13! 4! \cdots 3!} \right] \\ 
    &= -7 \lambda + (10+13+\cdots+3) \log(\lambda) - \left[ \log(10!) + \log(13!) + \cdots + \log(3!) \right]
\end{align}
We see that the last term of factorials does not involve our parameter value $\lambda$. 
This term is then a constant and we could instead aim to find the maximium of the simpler function 
\begin{align}
    \ell \ell (\lambda | \mathcal{D}) &= -7 \lambda + (10+13+\cdots+3) \log(\lambda) \\ 
    &=  -7 \lambda + 83\log(\lambda)
\end{align}

In some cases, we may be able to find the parameter value, or set of parameter values, that maximize the log likelihood function. 
To explore this topic we will need to briefly learn about the derivative of a continuous function. 

\section{ Just in time derivatives }

The derivative describes the tangential slope of a function at a point and can serve as a linear approximation to a more complicated function.
We will see how we can use the derivative to setup and solve an equation that will return a parameter value that maximizes the log likelihood. 
This estimate of our parameter will be called a \textbf{maximum likelihood estimate}.

\subsection{Sequences and limits}

A sequence is defined as a function whose domain is the natural numbers $\mathbb{N} = \{1,2,3,4,5,6, \cdots  \}$. 
Often as annotate the value associated with the 1, $f(1)$ as $a_{1}$, the value associated with 2, $f(2)$, as $a_{2}$, and so on.

\ex The following is a sequence $f(x) = 2^{-x}$ for $x \in \mathbb{N}$.The first few values of this sequence are $f(1) = a_{1} = 1/2$, $f(2) = a_{2} = 1/4$, $f(3) = a_{3} = 1/8$, and so on. We can also describe the above sequence using this notation $\{ 2^{-n} \}_{n=1}^{\infty}$.  

Intuitively, we may wish to know how a sequence behaves as it is enumerated. 
A sequence may increase to infinity, decrease to negative infinity, oscillate or have some other periodic behavior, or a sequence may move closer and closer to a single value. 
If a sequence ${a_{n}}_{n=1}^{\infty}$ gets closer and closer to a single value $l$ then we say that the sequence has a \textbf{limit} and that limit is the value $l$.

The more formal definition of a limit is the following:
For a sequence $\{a_{n}\}_{n=1}^{\infty}$, given any value $\epsilon >0$ the value $l$ is a limit if you can find an integer $N$ such that $|a_{N} - l| < \epsilon$ for integers greater than $N$ (i.e. $N+1$, $N+2$, $\cdots$).

We may also define a limit $L$ of a function at the value $a$ if given a sequence of values $a_{1}, a_{2}, \cdots a_{n}$ that approach the value $a$, the sequence $f(a_{1}), f(a_{2}), f(a_{3}), \cdots$ approaches the value $L$. We call $L$ the limit of $f$ at the value $a$ and write \begin{equation}
    \lim_{x \to a} f(x) = L
\end{equation}
Intuitively, the above says that as the input of our function approaches the value $a$, the function values approach the value $L$.

To solve our problem of finding the parameter value that maximizes the log likelihood, we need to learn about a special type of limit called the derivative.

\subsection{Derivative of a function}

The derivative of a function at the value $a$, or $f'(a)$, is 
\begin{align}
    f'(a) = \lim_{ x \to a } \frac{f(x) - f(a)}{x-a}
\end{align}

For intuition, we can imagine building a sequence
\begin{equation}
    a_{1},a_{2},a_{3},\cdots, a_{n} \to a
\end{equation} 
that approaches the value $a$ and computing the associated sequence
\begin{equation}
\frac{f(a_{1}) - f(a)}{a_{1}-a},\frac{f(a_{2}) - f(a)}{a_{2}-a},\frac{f(a_{3}) - f(a)}{a_{3}-a},\cdots,\frac{f(a_{n}) - f(a)}{a_{n}-a} \to f'(a)
\end{equation}

One common sequence that approaches $a$ is the sequence $a + \{h_{n}\}_{n=1}^{\infty}$ and where the sequence $h$ approaches $0$.
Then the definition of the derivative would be 
\begin{align}
    f'(a) &= \lim_{ h \to 0 } \frac{f(a+h) - f(a)}{a+h-a} \\ 
          &= \lim_{ h \to 0 } \frac{f(a+h) - f(a)}{h} 
\end{align}
This is because as the sequence $\{h_{n}\}$ approaches zero, the sequence $\{a+h_{n}\}$ approaches the value $a$.

\ex Let $g(x) = x^{2}$. The derivative of $g$ at the value 1 is by definition
\begin{align}
    g'(1) &= \lim_{ h \to 0 } \frac{g(1+h) - g(1) }{h} \\ 
          &= \lim_{ h \to 0 } \frac{ (1+h)^{2} - 1^{2} }{h}\\
          &= \lim_{ h \to 0 } \frac{ h^{2} + 2h + 1 - 1 }{h}\\
          &= \lim_{ h \to 0 } \frac{ h^{2} + 2h }{h}\\
          &= \lim_{ h \to 0 } \frac{ h + 2 }{1}\\
          &=2
\end{align}
For any value $c$ we find that the derivative of $g$ is 
\begin{align}
    g'(c) &= \lim_{ h \to 0 } \frac{g(c+h) - g(c) }{h} \\ 
          &= \lim_{ h \to 0 } \frac{ (c+h)^{2} - c^{2} }{h}\\
          &= \lim_{ h \to 0 } \frac{ h^{2} + 2hc + c^{2} - c^{2} }{h}\\
          &= \lim_{ h \to 0 } \frac{ h^{2} + 2hc }{h}\\
          &= \lim_{ h \to 0 } \frac{ h + 2c }{1}\\
          &=2c
\end{align}

In some sense, the derivative $g'(c)$ too is a function, the function $g'(c) = 2c$.

\subsection{When the derivative equals zero}

When the derivative of a function at the point $x^{*}$ is 0 the point $x^{*}$ is either a minimum or a maximum value. 
This is the key to finding a parameter value that maximizes our log likelihood. 
If we can compute the derivative of our log likelihood $\ell \ell'(\theta)$ and find the value $\theta$ such that $\ell \ell' = 0$ then we know we have found either a parameter value that minimizes or maximizes our log likelhood, which then minimizes or maximizes our likelihood, which suggests that this parameter value is the most plausible value given our observed data. Phew. 

Lets prove that when the derivative equal zero then we are at a maximum. 
\textit{Hypothesis}: If $f(x^{*}) > f(x)$ for any possible input $x$ then  $f'(x^{*})=0$.\\

Let us build a two sequences, one sequence $\{x_{1}, x_{2}, \cdots \}$ that approaches $x^{*}$ such that all the values are smaller than $x^{*}$ and a second sequence $\{y_{1}, y_{2}, \cdots \}$ such that all values are larger than $x^{*}$.

If we choose any $x_{n}$ in the sequence, the following must be true:
\begin{align}
    \frac{f(x_{n}) - f(x^{*})}{x_{n}-x^{*}} \geq 0 & \;\; &\text{bc $f(x^{*})$ is a max and } x_{n}< x^{*} \\ 
\end{align}

If we choose any $y_{n}$ in the sequence, the following must also be true:
\begin{align}
    \frac{f(y_{n}) - f(y^{*})}{y_{n}-x^{*}} \leq 0 & \;\; &\text{bc $f(x^{*})$ is a max and } y_{n}>y^{*} \\ 
\end{align}

If we assume that there exists a derivative value $f'(x^{*})$ then that value must be greater than or equal to zero and also less than or equal to zero. 
The only options then is $f'(x^{*}) = 0$.


\subsection{derivative "rules"}

There are several rules that can make computing derivatives of complicated functions easier. 

\begin{itemize}
    \item Constant Rule: if $f(x) = c$ where $c$ is a constant  then $f'(x) = 0$
        \begin{itemize}
           \item Example: if $f(x) = 72.67  $ then 
           \begin{align}
               f'(x) &= 0
           \end{align}
       \end{itemize}
       
    \item Power Rule: if $f(x) = x^{n}$ then $f'(x) = nx^{(n-1)}$ 
       \begin{itemize}
           \item Example: if $f(x) = x^{3/2}$ then $f'(x) = \frac{3}{2}x^{3/2-1} = \frac{3}{2}x^{1/2} $
       \end{itemize}
    \item Exp Rule: if $f(x) = e^{x}$ then $f'(x) = e^{x}$
    \item log Rule: if $f(x) = \log(x)$ then $f'(x) = \frac{1}{x}$
    \item Addition Rule: if $f(x) = g(x) + h(x)$ then $f'(x) = g'(x) + h'(x)$
       \begin{itemize}
           \item Example: if $f(x) = x^{2} + \frac{1}{3} x^{-1} $ then 
           \begin{align}
               f'(x) &= \left(x^{2}\right)' + \left( \frac{1}{3} x^{-1} \right)'\\
               f'(x) &= 2x^{1} +  \frac{-1}{3} x^{-2}
           \end{align}
       \end{itemize}
    \item "Chain" rule: if $f(x) = g( h(x) )$ then $ f'(x) = g'(h(x)) \cdot h'(x)$
        \begin{itemize}
            \item Example:~ Suppose $f(x) = (x+2)^{2}$ then g(h(x)) = $h(x)^{2}$ and $h(x) = x+2$.
            \begin{align}
                h'(x) &= 1\\
                g'(h(x)) &= 2 h(x)
            \end{align}
            Therefore 
            \begin{align}
                f'(x) &= g'(h(x)) \cdot h'(x)\\
                      &= 2 h(x) \cdot 1\\
                      &= 2(x+2)
            \end{align}
                    \item Example:~ Suppose $f(x) = \sqrt{ \log(x)}$ then g(h(x)) = $h(x)^{1/2}$ and $h(x) = \log(x)$.
            \begin{align}
                h'(x) &= \frac{1}{x}\\
                g'(h(x)) &= \frac{1}{2}h(x)^{-1/2}
            \end{align}
            Therefore 
            \begin{align}
                f'(x) &= g'(h(x)) \cdot h'(x)\\
                      &=  \frac{1}{2}h(x)^{-1/2} \cdot \frac{1}{x}\\
                      &= \frac{\log(x)^{-1/2}}{2x} \\ 
                      &= \frac{1}{2x \sqrt{\log(x)}}
            \end{align}
            
        \end{itemize}
    
      
\end{itemize}


\section{Computing the maximum likelihood estimator}

Given a i.i.d sample $(X_{1},X_{2},\cdots,X_{n})$ such that $X \sim f(x | \theta)$ we can compute a maximum likelihood estimator of the parameter $\theta$  by following these steps: (i) form the likelihood function (ii) convert the likelihood to a log likelihood (iii) compute the derivative of the log likelihood $\ell \ell ' (\theta)$ (iv) find the parameter value $\theta$ such that $\ell \ell ' (\theta) = 0$.

\ex Given $(X_{1},X_{2},X_{3}, \cdots, X_{n})$, the distribution for each $X_{i} \sim \text{Bern}(\theta)$, and the dataset $\mathcal{D} = (0,1,1,0,1)$ we can compute the maximum likelihood estimate for $\theta$. 
\begin{enumerate}
    \item The likelihood of $\mathcal{D}$ is
    \begin{align*}
        \ell(\theta) &= f(0) \cdot f(1) \cdot f(1) \cdot f(0) \cdot f(1)\\
                     &= \left(\theta^{x_{1}} (1-\theta)^{1-x_{1}}\right)
                        \left(\theta^{x_{2}} (1-\theta)^{1-x_{2}}\right)
                        \left(\theta^{x_{3}} (1-\theta)^{1-x_{3}}\right)
                        \left(\theta^{x_{4}} (1-\theta)^{1-x_{4}}\right)
                        \left(\theta^{x_{5}} (1-\theta)^{1-x_{5}}\right)\\
                    &=  \left(\theta^{0} (1-\theta)^{1-0}\right)
                        \left(\theta^{1} (1-\theta)^{1-1}\right)
                        \left(\theta^{1} (1-\theta)^{1-1}\right)
                        \left(\theta^{0} (1-\theta)^{1-0}\right)
                        \left(\theta^{1} (1-\theta)^{1-1}\right)\\
                    &=  \left( 1-\theta \right)
                        \left(\theta \right)
                        \left(\theta \right)
                        \left( 1-\theta\right)
                        \left(\theta \right)\\
                    &=  \theta^{3} (1-\theta)^{2}
    \end{align*} 
    \item Compute the log likelihood
    \begin{align*}
        \ell \ell (\theta) &= \log \left[ \ell(\theta)  \right] \\ 
                           &= \log \left[ \theta^{3} (1-\theta)^{2} \right] \\ 
                           &= \log \left[ \theta^{3}] + \log \left[ (1-\theta)^{2} \right] \\ 
                           &= 3 \log \left[\theta] + 2 \log \left[ (1-\theta)\right] \\ 
    \end{align*}
    \item Compute the derivative of the log likelihood 
    \begin{align*}
        \ell \ell (\theta)'  &= \left( 3 \log [\theta] + 2 \log [ (1-\theta)] \right)' \\ 
                             &= \left( 3 \log [\theta] \right)' + \left(2 \log [ (1-\theta)] \right)' & \text{Addition rule}\\
                             &= 3 \left( \log [\theta] \right)' + 2 \left( \log [ (1-\theta)] \right)' & \text{Constants rule}\\
                             &= 3 \frac{1}{\theta} + 2 \left( \log [ (1-\theta)] \right)' & \text{Derivative of log}\\
                             &= 3 \frac{1}{\theta} + 2 \left( \log [ z ] \right)' & \text{let } z=(1-\theta)\\
                             &= 3 \frac{1}{\theta} + 2 \frac{1}{z} (z)' & \text{Chain rule } \\
                             &= 3 \frac{1}{\theta} + 2 \frac{1}{1-\theta} (1-\theta)' & \text{Replace z} \\
                             &= 3 \frac{1}{\theta} + 2 \frac{1}{1-\theta} (1)' - (\theta)' & \text{Addition rule} \\
                             &= 3 \frac{1}{\theta} + 2 \frac{1}{1-\theta} 0 - (\theta)' & \text{Derivative of a constant} \\
                             &= 3 \frac{1}{\theta} + 2 \frac{1}{1-\theta} (-1) & \text{Derivative of } \theta \\
                             &= 3 \frac{1}{\theta} - 2 \frac{1}{1-\theta}\\
                             &= \frac{3}{\theta} -  \frac{2}{1-\theta}
    \end{align*}
    \item Set $\ell \ell(\theta)' = 0$ and solve for $\theta$
    \begin{align*}
        \ell \ell ' (\theta) &= \frac{3}{\theta} -  \frac{2}{1-\theta} = 0\\
        \frac{3}{\theta} &=  \frac{2}{1-\theta}\\
        3 - 3\theta &=  2 \theta \\
        3  &=  5 \theta \\
        \theta &= 3/5\\ 
        \hat {\theta} &= 3/5 
    \end{align*}

For our observed data $\mathcal{D}$ and our assumed model that the random variables generating each data point are i.i.d and distributed $\text{Bern}(\theta)$ our maximum likelihood estimate for $\theta$ is $\hat{\theta}_{\text{mle}} = 3/5$.

\end{enumerate}

\ex  Given $(Y_{1},Y_{2},Y_{3}, \cdots, Y_{n})$, the distribution for each $Y_{i} \sim \text{Geom}(p)$, and the dataset $\mathcal{D} = (7, 19, 1, 11)$ we can compute the maximum likelihood estimate for the parameter $p$. 
\begin{enumerate}
    \item The likelihood of $\mathcal{D}$ is
    \begin{align}
        \ell(p) &= f(9) \cdot f(19) \cdot f(1) \cdot f(11)\\
                &= p(1-p)^{8} \cdot p(1-p)^{18} \cdot p(1-p)^{0} \cdot p(1-p)^{10}\\
                &= p^{4} (1-p)^{8+18+0+10}\\
                &= p^{4} (1-p)^{36}\\
    \end{align}
    \item Form the log likelihood
    \begin{align}
        \ell \ell(p) &= \log\left[ \ell(p) \right] \\ 
                     &= \log \left[ p^{4} (1-p)^{36} \right] \\ 
                     &= \log \left[ p^{4} \right] + \log \left[(1-p)^{36} \right] \\
                     &= 4\log \left[ p \right] + 36\log \left[(1-p) \right] 
    \end{align}
    \item Compute the derivative of the log likelihood
    \begin{align}
        \ell \ell'(p) &= 4\frac{1}{p} + 36 \log( h(x) ) \text{ where } h(x) = 1-p\\
                      &= \frac{4}{p} + 36\frac{1}{h(x)} \cdot -1\\
                      &= \frac{4}{p} - 36\frac{1}{1-p} \\
                      &= \frac{4}{p} - \frac{36}{1-p}
    \end{align}
    \item Set the derivative equal to zero and solve for the parameter
    \begin{align}
        \ell \ell'(p)  &= \frac{4}{p} - \frac{36}{1-p} = 0 \\
                       &= \frac{ 4p(1-p) }{p} - \frac{36 p(1-p)}{1-p} = 0 \\ 
                       &=  4(1-p) - 36 p = 0 \\ 
                       &=  4 - 4p - 36 p = 0 \\ 
                       &=  4  = 40p \\ 
                     p &= 4/40 = 1/10  
    \end{align}
    
    For our observed data $\mathcal{D}$ and our assumed model that the random variables generating each data point are i.i.d and distributed $\text{Geom}(p)$, our maximum likelihood estimate for $p$ is $\hat{p}_{\text{mle}} = \frac{1}{10}$.

\end{enumerate}

\section{Fisher information}

A maximum likelihood estimate for a parameter describes what single parameter value most probably generated our observed dataset.
However, intuitively, if we had a collected a different dataset of the same size then we would likely have arrived at a different maximum likelihood estimate. 
Repeating the steps: (i) generate dataset of same size, (ii) compute maximum likelihood estimate, (iii) store estimate would lead to many estimates of the same model.  
To better characterize our model, we will need in addition to a single maximum likelihood estimate a measure of how that estimate may vary.

The Fisher Information $\mathcal{I}(\theta)$ about a parameter $\theta$ is defined as 
\begin{equation}
    \mathcal{I}(\theta) = \mathbb{E}\left( \left[ \ell'(x | \theta) \right]^{2} \right) = \mathbb{V}\left(\ell(\theta)\right)
\end{equation}
the intuition behind the Fisher information is that log likelihood function that are very peaked around their maximum likelihood estimate will have large Fisher information values. 
On the contrary, if a log-likelihood is very flat near it's maximum likelihood estimate then the Fisher information will be small.
 
In some sense, the Fisher information describes the curvature of the log likelihood around the maximum likelihood estimate and that curvature is also related to the variability of our estimate.

We will not explore this topic in class, but it is often easier to compute an equivalent definition of the Fisher information
\begin{equation}
    -\mathbb{E}( \ell \ell ''(\theta | X) ) = \mathbb{V}( \ell \ell ' (\theta | X)  ).
\end{equation}
This definition says that in order to compute the Fisher information we will need to compute the second derivative of the log likelihood, compute the expedctation, and then multiply that second derivative by negative one.

Lets look at an example of using this alternative definition to compute the Fisher information for a Bernoulli model.

\ex Suppose a sample $(X_{1},X_{2},X_{3},\cdots,X_{n})$ that we assume $X_{i} \sim \text{Bernoulli}(\theta)$. Then the log likelihood for a single data point is 
\begin{align}
    \ell(\theta) &= \log \left[ f(d_{1} | \theta) \right] \\ 
                &= \log \left[ \theta^{d_{1}}(1-\theta)^{1-d_{1}} \right] \\ 
                &=  d_{1}\log(\theta) + (1-d_{1})\log(1-\theta)  \\ 
\end{align}
and the derivative of the log likelihood is
\begin{align}
    \ell'(\theta) = \frac{d_{1}}{\theta} - \frac{1-d_{1}}{1-\theta} 
\end{align}
To compute the second derivative, we take the derivative of $\ell'(\theta)$. 
\begin{align}
    \ell'(\theta)   &= d_{1} \theta^{-1} - (1-d_{1})(1-\theta)^{-1} \\ 
    \ell''(\theta)  &= -d_{1} \theta^{-2} - (1-d_{1})(1-\theta)^{-2}
\end{align}
The above if the second derivative of the log likelihood for a single datapoint. 
However that data point was generated from a random variable $X_{1}$ and so we can then treat $ \ell''(\theta)$ as a transformation of the random variable $X_{1}$. 
\begin{align}
    \ell''(X | \theta)  &= -X \theta^{-2} - (1-X)(1-\theta)^{-2} \\ 
    \ell''(X | \theta)  &= - \frac{X}{\theta^{2}} - \frac{1-X}{ (1-\theta)^{2}} \\
\end{align}
The above transformation is a complicated function, but the fisher information only requires that we understand as single quantity from this transformation---the expected value. 
\begin{align}
    \mathbb{E}\left[\ell''(X | \theta)\right] &= - \frac{ \mathbb{E}(X)}{\theta^{2}} - \frac{1-\mathbb{E}(X)}{ (1-\theta)^{2}} \\ 
                                              &= -\frac{\theta}{\theta^{2}} - \frac{1-\theta}{ (1-\theta)^{2}} \\ 
                                              &= -\frac{1}{\theta} - \frac{1}{(1-\theta)}
\end{align}
The final steps asks that we take the negative of the quantity above 
\begin{align}
    \mathcal{I}(\theta | X) &=  \frac{1}{\theta} + \frac{1}{(1-\theta)} \\ 
                            &= \frac{ 1- \theta + \theta }{\theta(1-\theta)} \\ 
                            &= \frac{1}{\theta(1-\theta)}
\end{align}

\subsection{Fisher for a sample of size n}

The Fisher information for a sample of size n $(X_{1},X_{2},\cdots, X_{n})$ is equal to 
\begin{equation}
    \mathcal{I}(x_{1},x_{2},\cdots,x_{n}) = n \mathcal{I}(\theta)
\end{equation}
Intuitively, a sample of size $8$ is twice as informative as a sample of size $4$.

\section{CLT-like thm that involves the Fisher information}

An important theorem that will allow us to construct confidence intervals and hypothesis tests about our parameters states the following: 
\begin{equation}
    \sqrt{n}(\hat{\theta} - \theta) \sim \mathcal{N}\left(0,\frac{1}{\mathcal{I}(\theta)}\right)   
\end{equation}
where $\hat{\theta}$ is our maximum likelihood estimate, $\theta$ is our true, and unknown, parameter, and $\mathcal{I}$ is the information about $\theta$.

This theorem 


\section{Homework}

\begin{enumerate}
    \item Suppose that we collected a dataset with a single observation $\mathcal{D} = \{ 12 \}$. Please compute the probability of $\mathcal{D}$ if we assume our data was generated by a random variable $X$
    \begin{enumerate}
        \item Such that $X \sim \text{Geom}(p)$. Assume we know the parameter value $p$. 
        \item Such that $X \sim \text{Pois}(\lambda)$. Assume we know the parameter value $\lambda$. 
        \item Such that $X \sim \mathcal{N}(\mu,\sigma^{2})$.
        Assume we know the parameter values $(\mu, \sigma^{2})$.
    \end{enumerate}
    
    \item Suppose that we collected a dataset with a two observations $\mathcal{D} = \{ 12, 2 \}$. Please compute the likelihood function for $\mathcal{D}$ if we assume our data was generated by a random variable $X$
        \begin{enumerate}
        \item Such that $X \sim \text{Geom}(p)$. Assume we know the parameter value $p$. 
        \item Such that $X \sim \text{Pois}(\lambda)$ Assume we know the parameter value $\lambda$. 
        \item Such that $X \sim \text{Binom}(20, \theta)$.
        Assume we know the parameter value $\theta$.
    \end{enumerate}
    
    \item Suppose that we collected a dataset with a $n$ observations $\mathcal{D} = \{ x_{1}, x_{2}, \cdots, x_{n} \}$. Please compute the log likelihood if we assume our data was generated by a random variable $X$
        \begin{enumerate}
        \item Such that $X \sim \text{Geom}(p)$. Assume we know the parameter value $p$. 
        \item Such that $X \sim \text{Pois}(\lambda)$ Assume we know the parameter value $\lambda$. 
        \item Such that $X \sim \text{Binom}(N, \theta)$.
        Assume we know the parameter value $\theta$.
        \item Such that $X \sim \mathcal{N}(\mu,\sigma^{2})$.
        Assume we know the parameter values
    \end{enumerate}
    
    \item Suppose you collect a dataset $\mathcal{D} = (x_{1},x_{2},\cdots,x_{n})$ that you assume is generated from an i.i.d. sample $X_{1}, X_{2},\cdots, X_{n}$, where $X_{i} \sim f(x | \theta)$ and $\theta$ is a parameter.
    You decide to compute the loglikelihood
    \begin{align}
        \ell \ell (\theta | \mathcal{D}) = \sum_{i=1}^{n} \log \left [ f(x_{i} | \theta) \right]
    \end{align}
    Because we are interested in finding the value $\theta^{*}$ that maximizes $\mathcal{\ell \ell}$ then multiplying the log likelihood by a constant will not change the optimal $\theta^{*}$
    \begin{align}
        \frac{1}{n} \ell \ell (\theta | x_{1}, x_{2}, \cdots, x_{n}) = \frac{1}{n} \sum_{i=1}^{n} \log \left [ f(x_{i} | \theta) \right]
    \end{align}
    \begin{enumerate}
        \item The law of large numbers states that the above quantity will approach?   
    \end{enumerate}
    
    \item Suppose that we are asked to help better understand the burden of influenza circulating in in the state of Pennsylvania. 
    Over the course of 6 weeks we collect the following number of cases of reported influenza $\mathcal{D} = (143, 12, 124, 56, 66)$. 
    \begin{enumerate}
        \item Please propose a model for this data and a short description for why you chose this model. 
        \item Please compute the likelihood for your model parameters given the data
        \item Please compute the log likelihood.
    \end{enumerate}
    
    \item Consider the sequence $a_{n} = 2^{-n}$ for $n=1$ to $\infty$. 
    \begin{enumerate}
        \item Write down the first 3 items in this sequence 
        \item Please compute \[ \lim_{n \to \infty} a_{n} \]
    \end{enumerate}
    
    \item Does the infinite sequence $\{-1,1,-1,1,-1,1, \cdots \}$ have a limit? Why or why not? 
    
    \item Consider the finite sequence $a_{n} = 2^{-n}$ for $n=1$ to $100$, a sequence that ends at $2^{-100}$. 
    \begin{enumerate}
        \item Does this sequence have a limit point? Why or why not?
    \end{enumerate}
    
    \item Compute the derivative of 
    \begin{enumerate}
        \item $f(x) = e^{2x}$
        \item $f(x) = \log(\frac{1}{2x})$
        \item $f(x) = x^{2} + e^{-x}$
        \item $f(x) = 10$
        \item $f(x) = -3x$
        \item $f(x) = \frac{1}{2}x^{2}$
        \item $f(x) = \log(x)$
        \item $f(x) = \log(x^{2})$
        \item $f(x) = \log(10)$
    \end{enumerate}
    
    \item Consider the function $g(x) = -x^{2} + 3$. 
    \begin{enumerate}
        \item Compute the derivative of $g$
        \item Does the $x$ value that maximizes $g$ not depend on the value of $c$? Why or why not?
    \end{enumerate}

    \item Consider the function $g(x) = f(x) + c$ where $c$ is a constant. 
    \begin{enumerate}
        \item Compute the derivative of $g$
        \item Does the $x$ value that maximizes $g$ depend on the value of $c$? Why or why not?
    \end{enumerate}

    \item Consider the function $g(x) = cf(x)$ where $c$ is a constant. 
    \begin{enumerate}
        \item Compute the derivative of $g$
        \item Does the $x$ value that maximizes $g$ depend on the value of $c$? Why or why not?
    \end{enumerate}

    
    \item  Assume a set of random variables $Z_{1}, Z_{2}, \cdots, Z_{n}$ such that $Z_{i} \sim \text{exp}(\beta)$ generated an observed dataset. An exponential distribution assign a density over continuous numbers from 0 to infinity. The probability density function is 
    \begin{align*}
        f(z) = \beta e^{-\beta z}     
    \end{align*}
    Suppose we collect a dataset $\mathcal{D} = (2,4,0.5,3)$.
    \begin{enumerate}
        \item Please compute the log likelihood of an arbitrary dataset $\mathcal{D} = (z_{1},z_{2},z_{3}\cdots,z_{n})$
        \item Compute the derivative of the log likelihood above
        \item Set the derivative of the log likelihood equal to zero and solve for $\beta$ (ie find the parameter value that maximizes the log likelhiood). 
        \item Apply the maximum likelihood estimate you found to the dataset  $\mathcal{D} = (2,4,0.5,3)$.
    \end{enumerate}
    
    \item  Assume a set of random variables $Y_{1}, Y_{2}, \cdots, Y_{n}$ such that $Y_{i} \sim \text{Geom}(p)$ generated an observed dataset $\mathcal{D}$. Please compute the maximum likelihood estimate for $p$. Include the log likelihood, derivative, and solution when setting the derivative to zero: 
    \begin{enumerate}
        \item Given a dataset $\mathcal{D} = \{ 12, 1, 0, 3, 8  \}$
        \item Given a dataset $\mathcal{D} = \{y_{1}, y_{2}, \dots, y_{n}\}$
    \end{enumerate}
    
    \item Assume a set of random variables $A_{1}, A_{2}, \cdots, A_{n}$ such that $A_{i} \sim \text{Bern}(\theta)$ generated an observed dataset. Please compute the maximum likelihood estimate for $p$. Include the log likelihood, derivative, and solution when setting the derivative to zero: 
    \begin{enumerate}
        \item Given a dataset $\mathcal{D} = \{ 0,0,0,0 \}$
        \item Do you think the estimate you computed above is reasonable? Why or why not?
        \item Given a dataset $\mathcal{D} = \{a_{1}, a_{2}, \dots, a_{n}\}$
    \end{enumerate}
    
    \item Assume a set of random variables $A_{1}, A_{2}, \cdots, A_{n}$ such that $A_{i} \sim \text{Pois}(\lambda)$ generated an observed dataset. Please compute the maximum likelihood estimate for $p$. Include the log likelihood, derivative, and solution when setting the derivative to zero: 
    \begin{enumerate}
        \item Given a dataset $\mathcal{D} = \{ 11,12,2,10 \}$
        \item Given a dataset $\mathcal{D} = \{a_{1}, a_{2}, \dots, a_{n}\}$
    \end{enumerate}

    \item Assume that we model a sample $(X_{1},X_{2},\cdots,X_{n})$ with a Poisson distribution ($X \sim \text{Pois}(\lambda)$). Please derive the Fisher information $\mathcal{I}(\lambda)$ 
        
    \item The number of influenza A and influenza B cases each week is recorded by the Pennsylvania Department of health. 
    For the year 2022 the number of cases in Northampton county, PA is 
    \begin{table}[ht!]
        \centering
        \begin{tabular}{cc}
           Flu A & Flu B  \\
           \hline
            100 & 0 \\ 
            89 & 0 \\ 
            10 & 10 \\ 
            45 & 12 \\ 
            12 & 98 \\ 
        \end{tabular}
    \end{table}
    Please pose a model for the number of influenza A cases, and compute the maximum likelihood estimate for any parameters in this model. 
    Explain why you chose your particular model and explain parameter estimates.
    In addition, use the Fisher information about your parameter to build a 95CI around this estimated parameter value.
    
\end{enumerate}




